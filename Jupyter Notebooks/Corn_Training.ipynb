{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694xfIgODKOF",
        "outputId": "894d0ede-2400-49a4-d5fe-cb8a1244c8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLF8Pht3TFZJ",
        "outputId": "52a41e65-8f56-4a1f-ed0f-0994796769f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Validation', 'Training', 'Testing']\n"
          ]
        }
      ],
      "source": [
        "dir = os.listdir('drive/MyDrive/datasets/Corn')\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTT2oyL4DafO",
        "outputId": "0a108363-1343-4a7c-993d-352922cc1b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['blight', 'grey_leaf_spot', 'healthy', 'common_rust']\n"
          ]
        }
      ],
      "source": [
        "dir = os.listdir('drive/MyDrive/datasets/Corn/Training')\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7dtX9thDxG_",
        "outputId": "d42931d8-61ca-4ca4-fa1f-9b52eb531f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mnist_test.csv', 'mnist_train.csv', 'Colab Notebooks', 'Apple Leaf Disease', 'model', 'saved_sedensenet121_saved.keras', 'sedensenet121_after_part7.keras', 'sedensenet121_after_part8.keras', 'sedensenet121_after_part9.keras', 'sedensenet121_after_part10.keras', 'sedensenet121_after_part11.keras', 'datasets', 'best_custom_model1_part1.keras', 'inception_frozen_part1.keras', 'best_epoch90_inception_frozen_part1.keras', 'max_planck_weather_ts_modified.csv', 'best_epoch23_densenet121_part2.keras', 'Copy of best_epoch16_densenet121_part1.keras', 'Training_A3', 'Saved Models Apples', 'Saved Models Corn']\n"
          ]
        }
      ],
      "source": [
        "dir = os.listdir('drive/MyDrive')\n",
        "\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXl1wTYoDdBz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_images(dir):\n",
        "\n",
        "    supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
        "    image_list = []\n",
        "    count = 0\n",
        "    # Walk through the directory and read images\n",
        "    for root, _, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            file_extension = os.path.splitext(file)[-1].lower()\n",
        "\n",
        "            # Check if the file is a .jpg or .jpeg image\n",
        "            if file_extension in supported_extensions:\n",
        "                image_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    image = imageio.imread(image_path)\n",
        "                    image1 = image\n",
        "\n",
        "                    image = np.asarray(image)\n",
        "                    del image1\n",
        "                    image_list.append(image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading image {image_path}: {e}\")\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if count % 100 == 0:\n",
        "              print(str(count) + \" images read\")\n",
        "\n",
        "\n",
        "    return image_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WOMlfHOEDeKR",
        "outputId": "f7460548-ae95-422e-cc07-51f934c72c6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-2dac8990de03>:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(image_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 images read\n",
            "200 images read\n",
            "300 images read\n",
            "400 images read\n",
            "500 images read\n",
            "600 images read\n",
            "700 images read\n",
            "100 images read\n",
            "200 images read\n",
            "300 images read\n",
            "100 images read\n",
            "200 images read\n",
            "300 images read\n",
            "400 images read\n",
            "500 images read\n",
            "600 images read\n",
            "700 images read\n",
            "100 images read\n",
            "200 images read\n",
            "300 images read\n",
            "400 images read\n",
            "500 images read\n",
            "600 images read\n",
            "700 images read\n",
            "800 images read\n",
            "100 images read\n",
            "100 images read\n",
            "100 images read\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Train data directories\n",
        "base_train_dir = 'drive/MyDrive/datasets/Corn/Training'\n",
        "base_test_dir = 'drive/MyDrive/datasets/Corn/Validation'\n",
        "\n",
        "\n",
        "# Train data directories\n",
        "blight_train = read_images(os.path.join(base_train_dir, 'blight'))\n",
        "grey_leaf_spot_train = read_images(os.path.join(base_train_dir, 'grey_leaf_spot'))\n",
        "healthy_train = read_images(os.path.join(base_train_dir, 'healthy'))\n",
        "common_rust_train = read_images(os.path.join(base_train_dir, 'common_rust'))\n",
        "\n",
        "# Test data directories\n",
        "blight_test = read_images(os.path.join(base_test_dir, 'blight'))\n",
        "grey_leaf_spot_test = read_images(os.path.join(base_test_dir, 'grey_leaf_spot'))\n",
        "healthy_test = read_images(os.path.join(base_test_dir, 'healthy'))\n",
        "common_rust_test = read_images(os.path.join(base_test_dir, 'common_rust'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc04CAnpDnGK",
        "outputId": "2da9da6a-bcc3-4843-87df-9827c4ea2ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of blight_train: 779\n",
            "Length of grey_leaf_spot_train: 390\n",
            "Length of healthy_train: 790\n",
            "Length of common_rust_train: 888\n",
            "Length of blight_test: 137\n",
            "Length of grey_leaf_spot_test: 69\n",
            "Length of healthy_test: 139\n",
            "Length of common_rust_test: 156\n",
            "Total length of all train parts: 2847\n",
            "Total length of all test parts: 501\n",
            "Total length of all parts: 3348\n"
          ]
        }
      ],
      "source": [
        "# For training classes\n",
        "print(\"Length of blight_train:\", len(blight_train))\n",
        "print(\"Length of grey_leaf_spot_train:\", len(grey_leaf_spot_train))\n",
        "print(\"Length of healthy_train:\", len(healthy_train))\n",
        "print(\"Length of common_rust_train:\", len(common_rust_train))\n",
        "\n",
        "# For test classes\n",
        "print(\"Length of blight_test:\", len(blight_test))\n",
        "print(\"Length of grey_leaf_spot_test:\", len(grey_leaf_spot_test))\n",
        "print(\"Length of healthy_test:\", len(healthy_test))\n",
        "print(\"Length of common_rust_test:\", len(common_rust_test))\n",
        "\n",
        "# Calculate the lengths of all train parts\n",
        "train_lengths = len(blight_train) + len(grey_leaf_spot_train) + len(healthy_train) + len(common_rust_train)\n",
        "\n",
        "# Calculate the lengths of all test parts\n",
        "test_lengths = len(blight_test) + len(grey_leaf_spot_test) + len(healthy_test) + len(common_rust_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"Total length of all train parts:\", train_lengths)\n",
        "print(\"Total length of all test parts:\", test_lengths)\n",
        "\n",
        "# Calculate the total length of all parts\n",
        "total_length = train_lengths + test_lengths\n",
        "\n",
        "# Print the results\n",
        "print(\"Total length of all parts:\", total_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPaaxQYDqQM"
      },
      "outputs": [],
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import random\n",
        "import copy\n",
        "\n",
        "def add_gaussian_noise(images, mean_range=(0, 15), std_range=(0, 0.15)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.AdditiveGaussianNoise(loc=mean_range, scale=(0, 0.2*255))\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    # images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def random_crop(images, crop_percent=(0.1, 0.4)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Crop(percent=crop_percent)\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    # images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def random_rotate(images, rotation_range=(-360, 360)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Rotate(rotate=rotation_range)\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    # images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def invert_images(images):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Add(value=(-20, 20))\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    # images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "\n",
        "def adjust_brightness(images, brightness_range=(-65, 65)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline for adjusting brightness\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Multiply((1.0 + brightness_range[0] / 100.0, 1.0 + brightness_range[1] / 100.0))\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images_np]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "\n",
        "def scale_images(images, scale_factor = (0.3, 1.8)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline for scaling images\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Affine(scale=scale_factor)\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images_np]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def add_contrast(images, contrast_factor=(0.5, 1.5)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline for adding contrast\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.ContrastNormalization(alpha=contrast_factor)\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images_np]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "def flip_images(images, flip_probability=0.5):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline for randomly flipping images\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Sometimes(flip_probability, iaa.Fliplr(1.0)),  # Horizontal flips\n",
        "        iaa.Sometimes(flip_probability, iaa.Flipud(1.0))   # Vertical flips\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images_np]\n",
        "\n",
        "    return augmented_images\n",
        "\n",
        "\n",
        "# discard images according to ratio\n",
        "def discard_images(images, discard_ratio=0.5):\n",
        "    random.seed(10)\n",
        "    # Calculate the number of images to discard based on the discard_ratio\n",
        "    num_images_to_discard = int(len(images) * discard_ratio)\n",
        "\n",
        "    # Create a copy of the input list to avoid modifying the original list\n",
        "    remaining_images = images[:]\n",
        "\n",
        "    # Randomly discard a portion of the images\n",
        "    random.shuffle(remaining_images)\n",
        "    remaining_images = remaining_images[num_images_to_discard:]\n",
        "\n",
        "    return remaining_images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resize_images(images_list, width=128, height=128):\n",
        "    ia.seed(1)\n",
        "    # Define the resize augmentation\n",
        "    resize_augmenter = iaa.Resize({\"height\": height, \"width\": width})\n",
        "\n",
        "    resized_images = []\n",
        "\n",
        "    for image in images_list:\n",
        "        # Ensure the image is in RGB format (imgaug uses RGB by default)\n",
        "        if image.shape[-1] == 1:  # Grayscale image with single channel\n",
        "            image = np.repeat(image, 3, axis=-1)\n",
        "\n",
        "        # Apply the resize augmentation\n",
        "        augmented_image = resize_augmenter.augment_image(image)\n",
        "\n",
        "        # Append the augmented image to the result list\n",
        "        resized_images.append(augmented_image)\n",
        "\n",
        "    del images_list[:]\n",
        "    return resized_images\n",
        "\n",
        "def keep_n_images(images, n_to_keep):\n",
        "    random.seed(10)\n",
        "    if n_to_keep >= len(images):\n",
        "        return images  # Keep all images if n_to_keep is greater than or equal to the image count\n",
        "\n",
        "    # Randomly shuffle the images list\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # Keep the first n_to_keep images and discard the rest\n",
        "    kept_images = images[:n_to_keep]\n",
        "\n",
        "    # Create a copy of the kept images list\n",
        "    kept_images_copy = copy.deepcopy(kept_images)\n",
        "\n",
        "    # Clear the original images list to free memory\n",
        "    del images[:]\n",
        "\n",
        "    return kept_images_copy\n",
        "import cv2\n",
        "def normalize_images(image_list):\n",
        "\n",
        "  for i in range(len(image_list)):\n",
        "      image = image_list[i].astype(np.float32) / 255.0\n",
        "      image_list[i] = image\n",
        "\n",
        "def discard_images(lst, percent_to_discard):\n",
        "    # Randomly select images to discard using NumPy\n",
        "    num_images_to_discard = int(len(lst) * percent_to_discard)\n",
        "    indices_to_discard = np.random.choice(len(lst), size=num_images_to_discard, replace=False)\n",
        "\n",
        "    # Create a new list without the discarded images\n",
        "    modified_lst = np.delete(lst, indices_to_discard, axis=0)\n",
        "\n",
        "    return list(modified_lst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTks90BUDq62",
        "outputId": "b73b85b4-ff77-49ad-f68b-17a80d7564be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-162926356415>:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  images_np = np.array(images)\n",
            "<ipython-input-8-162926356415>:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  images_np = np.array(images)\n",
            "/usr/local/lib/python3.10/dist-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
            "  warn_deprecated(msg, stacklevel=3)\n",
            "<ipython-input-8-162926356415>:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  images_np = np.array(images)\n",
            "<ipython-input-8-162926356415>:122: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  images_np = np.array(images)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "blight_all = (\n",
        "    add_gaussian_noise(blight_train) +\n",
        "    random_crop(blight_train) +\n",
        "    invert_images(blight_train) +\n",
        "    adjust_brightness(blight_train) +\n",
        "    scale_images(blight_train) +\n",
        "    random_rotate(blight_train) +\n",
        "    add_contrast(blight_train) +\n",
        "    flip_images(blight_train) +\n",
        "    scale_images(blight_train) +\n",
        "    random_rotate(blight_train)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "grey_leaf_spot_all = (\n",
        "    add_gaussian_noise(grey_leaf_spot_train) +\n",
        "    random_crop(grey_leaf_spot_train) +\n",
        "    invert_images(grey_leaf_spot_train) +\n",
        "    adjust_brightness(grey_leaf_spot_train) +\n",
        "    scale_images(grey_leaf_spot_train) +\n",
        "    random_rotate(grey_leaf_spot_train) +\n",
        "    add_contrast(grey_leaf_spot_train) +\n",
        "    flip_images(grey_leaf_spot_train)  +\n",
        "    scale_images(grey_leaf_spot_train) +\n",
        "    random_rotate(grey_leaf_spot_train)\n",
        "\n",
        ")\n",
        "\n",
        "healthy_all = (\n",
        "    add_gaussian_noise(healthy_train) +\n",
        "    random_crop(healthy_train) +\n",
        "    invert_images(healthy_train) +\n",
        "    adjust_brightness(healthy_train) +\n",
        "    scale_images(healthy_train) +\n",
        "    random_rotate(healthy_train) +\n",
        "    add_contrast(healthy_train) +\n",
        "    flip_images(healthy_train) +\n",
        "    scale_images(healthy_train) +\n",
        "    random_rotate(healthy_train)\n",
        "    )\n",
        "\n",
        "\n",
        "common_rust_all = (\n",
        "    add_gaussian_noise(common_rust_train) +\n",
        "    random_crop(common_rust_train) +\n",
        "    invert_images(common_rust_train) +\n",
        "    adjust_brightness(common_rust_train) +\n",
        "    scale_images(common_rust_train) +\n",
        "    random_rotate(common_rust_train) +\n",
        "    add_contrast(common_rust_train) +\n",
        "    flip_images(common_rust_train) +\n",
        "    scale_images(common_rust_train) +\n",
        "    random_rotate(common_rust_train)\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaxJ-J5qk8QI"
      },
      "outputs": [],
      "source": [
        "blight_train = blight_train + blight_all\n",
        "grey_leaf_spot_train = grey_leaf_spot_train + grey_leaf_spot_all\n",
        "healthy_train = healthy_train + healthy_all\n",
        "common_rust_train = common_rust_train + common_rust_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBIUOdVHD1pD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "labels_blight_train = np.zeros(len(blight_train))\n",
        "labels_grey_leaf_spot_train = np.ones(len(grey_leaf_spot_train))\n",
        "labels_healthy_train = np.full(len(healthy_train), 2)\n",
        "labels_common_rust_train = np.full(len(common_rust_train), 3)\n",
        "\n",
        "# Combine train labels\n",
        "labels_train = np.concatenate([\n",
        "    labels_blight_train,\n",
        "    labels_grey_leaf_spot_train,\n",
        "    labels_healthy_train,\n",
        "    labels_common_rust_train\n",
        "])\n",
        "\n",
        "labels_blight_test = np.zeros(len(blight_test))\n",
        "labels_grey_leaf_spot_test = np.ones(len(grey_leaf_spot_test))\n",
        "labels_healthy_test = np.full(len(healthy_test), 2)\n",
        "labels_common_rust_test = np.full(len(common_rust_test), 3)\n",
        "\n",
        "# Combine test labels\n",
        "labels_test = np.concatenate([\n",
        "    labels_blight_test,\n",
        "    labels_grey_leaf_spot_test,\n",
        "    labels_healthy_test,\n",
        "    labels_common_rust_test\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBt4R0viD3Pz"
      },
      "outputs": [],
      "source": [
        "images_train = blight_train + grey_leaf_spot_train + healthy_train + common_rust_train\n",
        "images_test = blight_test + grey_leaf_spot_test + healthy_test + common_rust_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAICP4PbD6I2"
      },
      "outputs": [],
      "source": [
        "images_train = resize_images(images_train, 128,128)\n",
        "images_test = resize_images(images_test, 128, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm18BEYAxqG9"
      },
      "outputs": [],
      "source": [
        "\n",
        "normalize_images(images_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzUKhudfCMuR",
        "outputId": "3c35d115-5756-4376-f26e-366e44aee695"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0.5176471 , 0.5176471 , 0.5254902 ],\n",
              "        [0.5294118 , 0.5254902 , 0.54901963],\n",
              "        [0.5294118 , 0.52156866, 0.5568628 ],\n",
              "        ...,\n",
              "        [0.59607846, 0.58431375, 0.6117647 ],\n",
              "        [0.57254905, 0.56078434, 0.5882353 ],\n",
              "        [0.49411765, 0.48235294, 0.50980395]],\n",
              "\n",
              "       [[0.5254902 , 0.5254902 , 0.53333336],\n",
              "        [0.5254902 , 0.52156866, 0.54509807],\n",
              "        [0.5294118 , 0.5176471 , 0.5529412 ],\n",
              "        ...,\n",
              "        [0.5803922 , 0.5686275 , 0.59607846],\n",
              "        [0.5921569 , 0.5803922 , 0.60784316],\n",
              "        [0.5686275 , 0.5568628 , 0.5803922 ]],\n",
              "\n",
              "       [[0.52156866, 0.52156866, 0.5294118 ],\n",
              "        [0.52156866, 0.50980395, 0.5372549 ],\n",
              "        [0.5294118 , 0.5176471 , 0.56078434],\n",
              "        ...,\n",
              "        [0.5882353 , 0.5764706 , 0.6117647 ],\n",
              "        [0.59607846, 0.58431375, 0.62352943],\n",
              "        [0.5882353 , 0.5764706 , 0.6117647 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.41568628, 0.40392157, 0.43137255],\n",
              "        [0.4392157 , 0.42745098, 0.45490196],\n",
              "        [0.42745098, 0.41568628, 0.44313726],\n",
              "        ...,\n",
              "        [0.35686275, 0.2901961 , 0.1882353 ],\n",
              "        [0.27058825, 0.23921569, 0.1254902 ],\n",
              "        [0.44313726, 0.43137255, 0.30980393]],\n",
              "\n",
              "       [[0.42745098, 0.41568628, 0.44313726],\n",
              "        [0.43529412, 0.42352942, 0.4509804 ],\n",
              "        [0.42352942, 0.4117647 , 0.4392157 ],\n",
              "        ...,\n",
              "        [0.43137255, 0.3529412 , 0.2627451 ],\n",
              "        [0.36078432, 0.3137255 , 0.21176471],\n",
              "        [0.2509804 , 0.23137255, 0.11372549]],\n",
              "\n",
              "       [[0.41568628, 0.40392157, 0.43137255],\n",
              "        [0.41960785, 0.40784314, 0.43529412],\n",
              "        [0.41960785, 0.40784314, 0.43529412],\n",
              "        ...,\n",
              "        [0.46666667, 0.38039216, 0.2901961 ],\n",
              "        [0.36862746, 0.3137255 , 0.21568628],\n",
              "        [0.2784314 , 0.24705882, 0.13333334]]], dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images_train[80]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVOutQGDxwfT"
      },
      "outputs": [],
      "source": [
        "normalize_images(images_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKXb38zpD6tC"
      },
      "outputs": [],
      "source": [
        "#TEST Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X125FDBD9Pd",
        "outputId": "b7d3f1ef-a11b-4c69-b687-dc8c6dc67785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31317\n",
            "31317\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[8569, 4290, 8690, 9768]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(images_train))\n",
        "print(len(labels_train))\n",
        "\n",
        "counts = [0,0,0,0]\n",
        "\n",
        "for label in labels_train:\n",
        "    counts[int(label)] += 1\n",
        "\n",
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS_dvGp1D_zT"
      },
      "outputs": [],
      "source": [
        "\n",
        "ls1 = images_test\n",
        "ls2 = images_train\n",
        "\n",
        "images_test = np.array(images_test)\n",
        "images_train = np.array(images_train)\n",
        "\n",
        "del ls1[:]\n",
        "del ls2[:]\n",
        "\n",
        "shuffle_indices_train = np.random.permutation(len(images_train))\n",
        "shuffle_indices_test = np.random.permutation(len(images_test))\n",
        "\n",
        "\n",
        "images_train = images_train[shuffle_indices_train]\n",
        "labels_train = labels_train[shuffle_indices_train]\n",
        "\n",
        "images_test = images_test[shuffle_indices_test]\n",
        "labels_test = labels_test[shuffle_indices_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NsYhuwREBCD"
      },
      "outputs": [],
      "source": [
        "# %load custom_callback.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Find accuracy of model\n",
        "def find_accuracy(test,pred):\n",
        "    correct = 0\n",
        "    total = len(test)\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        if test[i] == pred[i]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "\n",
        "# Map ANN outputs to classes\n",
        "def get_labels(y_pred_ann):\n",
        "    labels = []\n",
        "\n",
        "    for pred in y_pred_ann:\n",
        "        max_index = 0\n",
        "\n",
        "        for i in range(len(pred)):\n",
        "            if pred[i] > pred[max_index]:\n",
        "                max_index = i\n",
        "\n",
        "        labels.append(max_index)\n",
        "\n",
        "    return labels\n",
        "\n",
        "# This callback prints accuracy by epoch information after each epoch\n",
        "class Save_Accuracy_By_Epoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.X_Test = test_data[0]\n",
        "        self.Y_Test = test_data[1]\n",
        "        self.accuracies = []\n",
        "        self.epochs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        y_pred = self.model.predict(self.X_Test)\n",
        "\n",
        "        if epoch == 4:\n",
        "            pass\n",
        "\n",
        "        y_pred = get_labels(y_pred)\n",
        "        accuracy = find_accuracy(self.Y_Test, y_pred)\n",
        "        self.epochs.append(epoch+1)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        print(self.epochs)\n",
        "        print(self.accuracies)\n",
        "\n",
        "\n",
        "# This callback prints metrics for every class after each epoch\n",
        "class Save_Multiclass_Metrics_By_Epoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_data, n_classes, save_after = 10, save_csv_path = 'results.csv', model_name = 'model.keras'):\n",
        "        self.X_Test = test_data[0]\n",
        "        self.Y_Test = test_data[1]\n",
        "        self.epochs = []\n",
        "        self.n_classes = n_classes\n",
        "        self.save_after = save_after\n",
        "        self.save_csv_path = save_csv_path\n",
        "        self.model_name = model_name\n",
        "        self.max_accuracy = 0\n",
        "\n",
        "        self.mat_sensitivity = []\n",
        "        self.mat_specificity = []\n",
        "        self.mat_precision = []\n",
        "        self.mat_recall = []\n",
        "        self.mat_accuracy = []\n",
        "        self.mat_f1 = []\n",
        "        self.accuracies = []\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            self.mat_sensitivity.append([])\n",
        "            self.mat_specificity.append([])\n",
        "            self.mat_precision.append([])\n",
        "            self.mat_recall.append([])\n",
        "            self.mat_accuracy.append([])\n",
        "            self.mat_f1.append([])\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        y_pred = self.model.predict(self.X_Test)\n",
        "        y_pred = get_labels(y_pred)\n",
        "\n",
        "        total = len(self.Y_Test)\n",
        "\n",
        "        correct = 0\n",
        "        for i in range(len(y_pred)):\n",
        "            if y_pred[i] == self.Y_Test[i]:\n",
        "                correct += 1\n",
        "\n",
        "        accuracy = correct/total\n",
        "        self.accuracies.append(correct / total)\n",
        "\n",
        "        best = False\n",
        "\n",
        "        if accuracy >= self.max_accuracy:\n",
        "            self.max_accuracy = accuracy\n",
        "            best = True\n",
        "\n",
        "\n",
        "        if accuracy > 0.95:\n",
        "          best = True\n",
        "\n",
        "        for i in range(self.n_classes):\n",
        "            TP = 0\n",
        "            FP = 0\n",
        "            TN = 0\n",
        "            FN = 0\n",
        "\n",
        "            for j in range(len(y_pred)):\n",
        "                if self.Y_Test[j] == i and y_pred[j] == i:\n",
        "                    TP += 1\n",
        "                elif self.Y_Test[j] != i and y_pred[j] == i:\n",
        "                    FP += 1\n",
        "                elif self.Y_Test[j] == i and y_pred[j] != i:\n",
        "                    FN += 1\n",
        "                elif self.Y_Test[j] != i and y_pred[j] != i:\n",
        "                    TN += 1\n",
        "\n",
        "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
        "            specificity = TN / (TN + FP) if (TN + FP) > 0 else -1\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else -1\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
        "            accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else -1\n",
        "\n",
        "            self.mat_sensitivity[i].append(sensitivity)\n",
        "            self.mat_specificity[i].append(specificity)\n",
        "            self.mat_precision[i].append(precision)\n",
        "            self.mat_recall[i].append(recall)\n",
        "            self.mat_accuracy[i].append(accuracy)\n",
        "            self.mat_f1[i].append(f1)\n",
        "\n",
        "        self.epochs.append(int(epoch+1))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "           from keras import backend as K\n",
        "           print('learning rate changed from ', end = '')\n",
        "           print(self.model.optimizer.learning_rate, end = '')\n",
        "           K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * 0.9)\n",
        "           print(' to ', end = '')\n",
        "           print(self.model.optimizer.learning_rate)\n",
        "\n",
        "        if (epoch + 1) % self.save_after == 0:\n",
        "            save_to_csv_file(self.save_csv_path, self.mat_sensitivity, self.mat_specificity, self.mat_precision, self.mat_recall, self.mat_accuracy, self.mat_f1, self.accuracies, self.epochs)\n",
        "            self.model.save(self.model_name)\n",
        "            print(\"max Accuracy: \", self.max_accuracy)\n",
        "            if best:\n",
        "                self.model.save(\"best_epoch\" + str((epoch+1)) + \"_\" + self.model_name)\n",
        "\n",
        "            pass\n",
        "\n",
        "def save_to_csv_file(path, mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1, accuracies, epochs):\n",
        "    mat_sensitivity = np.transpose(mat_sensitivity)\n",
        "    mat_specificity = np.transpose(mat_specificity)\n",
        "    mat_precision = np.transpose(mat_precision)\n",
        "    mat_recall = np.transpose(mat_recall)\n",
        "    mat_accuracy = np.transpose(mat_accuracy)\n",
        "    mat_f1 = np.transpose(mat_f1)\n",
        "    accuracies = np.reshape(accuracies, (-1,1))\n",
        "    epochs = np.reshape(epochs, (-1,1))\n",
        "\n",
        "    mat_join = np.concatenate((epochs,accuracies,mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1), axis = 1)\n",
        "\n",
        "    n = mat_sensitivity.shape[1]\n",
        "\n",
        "\n",
        "    col_array_sensitivity = list(range(n))\n",
        "    col_array_specificity = list(range(n))\n",
        "    col_array_precision = list(range(n))\n",
        "    col_array_recall = list(range(n))\n",
        "    col_array_accuracy = list(range(n))\n",
        "    col_array_f1 = list(range(n))\n",
        "\n",
        "\n",
        "    for i in range(n):\n",
        "        col_array_sensitivity[i] = 'sensitivity Class ' + str(col_array_sensitivity[i])\n",
        "        col_array_specificity[i] = 'specificity Class ' + str(col_array_specificity[i])\n",
        "        col_array_precision[i] = 'precision Class ' + str(col_array_precision[i])\n",
        "        col_array_recall[i] = 'recall Class ' + str(col_array_recall[i])\n",
        "        col_array_accuracy[i] = 'accuracy Class ' + str(col_array_accuracy[i])\n",
        "        col_array_f1[i] = 'f1 Class ' + str(col_array_f1[i])\n",
        "\n",
        "    cols = ['Epoch']+['overall_accuracy']+ col_array_sensitivity + col_array_specificity + col_array_precision + col_array_recall + col_array_accuracy + col_array_f1\n",
        "\n",
        "    mat_join = np.flip(mat_join, axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        columns = cols,\n",
        "        data  = mat_join\n",
        "    )\n",
        "\n",
        "    df.to_csv(path, index= False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ewhCiGnECZl"
      },
      "outputs": [],
      "source": [
        "# One hot encoding\n",
        "labels_train_encoded = []\n",
        "for label in labels_train:\n",
        "    encoding = [0,0,0,0]\n",
        "    encoding[int(label)] = 1\n",
        "    labels_train_encoded.append(encoding)\n",
        "\n",
        "labels_train_encoded = np.array(labels_train_encoded)\n",
        "\n",
        "labels_test_encoded = []\n",
        "for label in labels_test:\n",
        "    encoding = [0,0,0,0]\n",
        "    encoding[int(label)] = 1\n",
        "    labels_test_encoded.append(encoding)\n",
        "\n",
        "labels_test_encoded = np.array(labels_test_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lac-X6wtot2p"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "raw_model = tf.keras.applications.Xception(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(128,128,3),\n",
        "    pooling=None\n",
        ")\n",
        "\n",
        "model.add(raw_model)\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D65LqJMLpa75",
        "outputId": "822f0c2e-7870-4865-f727-56d5c38321f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (Functional)          (None, 64)                167520    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 64)                256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               16640     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 218852 (854.89 KB)\n",
            "Trainable params: 216684 (846.42 KB)\n",
            "Non-trainable params: 2168 (8.47 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# class_weights = [2.18, 1.459, 0.719, 0.259, 7.64, 2.0459]\n",
        "# [cedar rust, general scab, grey spot, healthy, serious cedar rust, serious scab]\n",
        "\n",
        "#[healthy, general scab, serious scab, grey spot, genral cedar rust, serious cedar rust]\n",
        "# class_weights = [0.259, 1.459, 2.0459, 0.719, 2.18, 7.64]\n",
        "\n",
        "\n",
        "\n",
        "loss = tf.keras.losses.CategoricalFocalCrossentropy(\n",
        "    alpha=0.25,\n",
        "    gamma=0.1,\n",
        "    from_logits=False,\n",
        "    label_smoothing=0.0,\n",
        "    axis=-1,\n",
        "    name='categorical_focal_crossentropy'\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "tf.keras.utils.set_random_seed(2)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "tf.keras.utils.set_random_seed(21)\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=opt, loss=loss, metrics = ['acc'])\n",
        "# model.compile(optimizer='adam', loss=loss, metrics = ['acc'], loss_weights=class_weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYya-q-9BNKB",
        "outputId": "f94d2482-ffac-4d00-ece2-a39d7a64bf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "16/16 [==============================] - 1s 20ms/step\n",
            "max Accuracy:  0.590818363273453\n",
            "245/245 [==============================] - 43s 95ms/step - loss: 0.8755 - acc: 0.6670 - val_loss: 0.9724 - val_acc: 0.5908\n",
            "Epoch 2/500\n",
            "16/16 [==============================] - 0s 11ms/step\n",
            "max Accuracy:  0.8642714570858283\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.5245 - acc: 0.8058 - val_loss: 0.3642 - val_acc: 0.8643\n",
            "Epoch 3/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.874251497005988\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.4040 - acc: 0.8545 - val_loss: 0.3070 - val_acc: 0.8743\n",
            "Epoch 4/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9141716566866267\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.3476 - acc: 0.8763 - val_loss: 0.2666 - val_acc: 0.9142\n",
            "Epoch 5/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9141716566866267\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.3164 - acc: 0.8890 - val_loss: 0.2652 - val_acc: 0.9062\n",
            "Epoch 6/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9181636726546906\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.2938 - acc: 0.8983 - val_loss: 0.2146 - val_acc: 0.9182\n",
            "Epoch 7/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9401197604790419\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.2674 - acc: 0.9093 - val_loss: 0.1990 - val_acc: 0.9401\n",
            "Epoch 8/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9401197604790419\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.2517 - acc: 0.9146 - val_loss: 0.2065 - val_acc: 0.9401\n",
            "Epoch 9/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9481037924151696\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.2486 - acc: 0.9153 - val_loss: 0.1741 - val_acc: 0.9481\n",
            "Epoch 10/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "learning rate changed from <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04> to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=8.9999994e-05>\n",
            "max Accuracy:  0.9481037924151696\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.2295 - acc: 0.9219 - val_loss: 0.3279 - val_acc: 0.8982\n",
            "Epoch 11/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 82ms/step - loss: 0.2201 - acc: 0.9270 - val_loss: 0.1671 - val_acc: 0.9521\n",
            "Epoch 12/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.2177 - acc: 0.9276 - val_loss: 0.2066 - val_acc: 0.9301\n",
            "Epoch 13/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.2077 - acc: 0.9304 - val_loss: 0.1875 - val_acc: 0.9321\n",
            "Epoch 14/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1986 - acc: 0.9351 - val_loss: 0.2122 - val_acc: 0.9341\n",
            "Epoch 15/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1958 - acc: 0.9351 - val_loss: 0.1927 - val_acc: 0.9401\n",
            "Epoch 16/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9520958083832335\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1887 - acc: 0.9374 - val_loss: 0.1719 - val_acc: 0.9481\n",
            "Epoch 17/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1833 - acc: 0.9396 - val_loss: 0.1527 - val_acc: 0.9541\n",
            "Epoch 18/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1738 - acc: 0.9433 - val_loss: 0.1768 - val_acc: 0.9461\n",
            "Epoch 19/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1777 - acc: 0.9412 - val_loss: 0.1786 - val_acc: 0.9521\n",
            "Epoch 20/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "learning rate changed from <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=8.9999994e-05> to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=8.099999e-05>\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1666 - acc: 0.9447 - val_loss: 0.1890 - val_acc: 0.9461\n",
            "Epoch 21/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1636 - acc: 0.9480 - val_loss: 0.1527 - val_acc: 0.9481\n",
            "Epoch 22/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9540918163672655\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1590 - acc: 0.9497 - val_loss: 0.1698 - val_acc: 0.9461\n",
            "Epoch 23/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9600798403193613\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1553 - acc: 0.9508 - val_loss: 0.1575 - val_acc: 0.9601\n",
            "Epoch 24/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9600798403193613\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1537 - acc: 0.9522 - val_loss: 0.1679 - val_acc: 0.9541\n",
            "Epoch 25/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9600798403193613\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1482 - acc: 0.9546 - val_loss: 0.2268 - val_acc: 0.9321\n",
            "Epoch 26/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9600798403193613\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1510 - acc: 0.9525 - val_loss: 0.1642 - val_acc: 0.9541\n",
            "Epoch 27/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1421 - acc: 0.9547 - val_loss: 0.1404 - val_acc: 0.9641\n",
            "Epoch 28/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1399 - acc: 0.9567 - val_loss: 0.1682 - val_acc: 0.9481\n",
            "Epoch 29/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1348 - acc: 0.9592 - val_loss: 0.1834 - val_acc: 0.9501\n",
            "Epoch 30/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "learning rate changed from <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=8.099999e-05> to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.289999e-05>\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1366 - acc: 0.9571 - val_loss: 0.1463 - val_acc: 0.9541\n",
            "Epoch 31/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1304 - acc: 0.9607 - val_loss: 0.2007 - val_acc: 0.9421\n",
            "Epoch 32/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1316 - acc: 0.9610 - val_loss: 0.1400 - val_acc: 0.9601\n",
            "Epoch 33/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1284 - acc: 0.9614 - val_loss: 0.1758 - val_acc: 0.9541\n",
            "Epoch 34/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1243 - acc: 0.9628 - val_loss: 0.1550 - val_acc: 0.9501\n",
            "Epoch 35/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1225 - acc: 0.9647 - val_loss: 0.1708 - val_acc: 0.9561\n",
            "Epoch 36/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1239 - acc: 0.9634 - val_loss: 0.1588 - val_acc: 0.9641\n",
            "Epoch 37/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1165 - acc: 0.9664 - val_loss: 0.2360 - val_acc: 0.9461\n",
            "Epoch 38/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1191 - acc: 0.9654 - val_loss: 0.2115 - val_acc: 0.9421\n",
            "Epoch 39/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1154 - acc: 0.9662 - val_loss: 0.1806 - val_acc: 0.9561\n",
            "Epoch 40/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "learning rate changed from <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=7.289999e-05> to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.560999e-05>\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1107 - acc: 0.9686 - val_loss: 0.1878 - val_acc: 0.9541\n",
            "Epoch 41/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1088 - acc: 0.9694 - val_loss: 0.1807 - val_acc: 0.9541\n",
            "Epoch 42/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1117 - acc: 0.9677 - val_loss: 0.1620 - val_acc: 0.9601\n",
            "Epoch 43/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1095 - acc: 0.9685 - val_loss: 0.1614 - val_acc: 0.9541\n",
            "Epoch 44/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1043 - acc: 0.9711 - val_loss: 0.1952 - val_acc: 0.9481\n",
            "Epoch 45/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1031 - acc: 0.9706 - val_loss: 0.1855 - val_acc: 0.9501\n",
            "Epoch 46/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 80ms/step - loss: 0.1038 - acc: 0.9697 - val_loss: 0.2157 - val_acc: 0.9481\n",
            "Epoch 47/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1020 - acc: 0.9716 - val_loss: 0.1681 - val_acc: 0.9521\n",
            "Epoch 48/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1003 - acc: 0.9730 - val_loss: 0.1598 - val_acc: 0.9641\n",
            "Epoch 49/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.0986 - acc: 0.9728 - val_loss: 0.1759 - val_acc: 0.9521\n",
            "Epoch 50/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "learning rate changed from <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=6.560999e-05> to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=5.9048987e-05>\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.1010 - acc: 0.9715 - val_loss: 0.1652 - val_acc: 0.9581\n",
            "Epoch 51/500\n",
            "16/16 [==============================] - 0s 10ms/step\n",
            "max Accuracy:  0.9640718562874252\n",
            "245/245 [==============================] - 20s 81ms/step - loss: 0.0938 - acc: 0.9750 - val_loss: 0.1766 - val_acc: 0.9541\n",
            "Epoch 52/500\n",
            "212/245 [========================>.....] - ETA: 2s - loss: 0.0961 - acc: 0.9741"
          ]
        }
      ],
      "source": [
        "history = model.fit(images_train, labels_train_encoded, batch_size=128, epochs=500, validation_data=(images_test, labels_test_encoded), callbacks=[Save_Multiclass_Metrics_By_Epoch((images_test,labels_test), n_classes=4, save_after=1, save_csv_path=\"results_xception_part1.csv\", model_name=\"xception_part1.keras\")])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOI7w2C0TPWh"
      },
      "outputs": [],
      "source": [
        "# from keras import backend as K\n",
        "# K.set_value(model.optimizer.learning_rate, 0.00005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8PsyhBY9J0O"
      },
      "outputs": [],
      "source": [
        "# model.save('drive/MyDrive/Saved Models Corn/best_epoch175_customModel_part1.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
