{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "\n",
    "# # New Google Drive shared link\n",
    "# url = 'https://drive.google.com/uc?id=1Uh0kG-rFieKnF7bkukZbpubBTqd8_eTi'\n",
    "# output = 'TomatoV2.zip'  # Path where the new file will be saved\n",
    "\n",
    "# # Download the new file\n",
    "# gdown.download(url, output, quiet=False)\n",
    "\n",
    "# print(f\"File has been successfully downloaded to: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7dtX9thDxG_",
    "outputId": "d42931d8-61ca-4ca4-fa1f-9b52eb531f08"
   },
   "outputs": [],
   "source": [
    "# import zipfile as zf\n",
    "# files = zf.ZipFile(\"TomatoV2.zip\", 'r')\n",
    "# files.extractall('Tomato')\n",
    "# files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Specify the directory path\n",
    "# directory_path = 'VGG16'\n",
    "\n",
    "# # List all files in the directory\n",
    "# for filename in os.listdir(directory_path):\n",
    "#     file_path = os.path.join(directory_path, filename)\n",
    "#     try:\n",
    "#         # Check if it is a file\n",
    "#         if os.path.isfile(file_path):\n",
    "#             os.remove(file_path)\n",
    "#             print(f\"{file_path} has been deleted.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error deleting {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QXl1wTYoDdBz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 07:23:58.533149: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 07:23:58.533201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 07:23:58.533228: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 07:23:58.541735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_images(dir):\n",
    "\n",
    "    supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "    image_list = []\n",
    "    count = 0\n",
    "    # Walk through the directory and read images\n",
    "    for root, _, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            file_extension = os.path.splitext(file)[-1].lower()\n",
    "\n",
    "            # Check if the file is a .jpg or .jpeg image\n",
    "            if file_extension in supported_extensions:\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = imageio.imread(image_path)\n",
    "                    image1 = image\n",
    "\n",
    "                    image = np.asarray(image)\n",
    "                    del image1\n",
    "                    image_list.append(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading image {image_path}: {e}\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count % 100 == 0:\n",
    "              print(str(count) + \" images read\")\n",
    "\n",
    "\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOMlfHOEDeKR",
    "outputId": "f7460548-ae95-422e-cc07-51f934c72c6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/1632636461.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(image_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "1200 images read\n",
      "1300 images read\n",
      "1400 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "1200 images read\n",
      "1300 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "100 images read\n",
      "200 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "1200 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "1200 images read\n",
      "1300 images read\n",
      "1400 images read\n",
      "1500 images read\n",
      "1600 images read\n",
      "1700 images read\n",
      "1800 images read\n",
      "1900 images read\n",
      "2000 images read\n",
      "2100 images read\n",
      "2200 images read\n",
      "2300 images read\n",
      "2400 images read\n",
      "2500 images read\n",
      "2600 images read\n",
      "2700 images read\n",
      "2800 images read\n",
      "2900 images read\n",
      "3000 images read\n",
      "3100 images read\n",
      "3200 images read\n",
      "3300 images read\n",
      "3400 images read\n",
      "3500 images read\n",
      "3600 images read\n",
      "3700 images read\n",
      "100 images read\n",
      "200 images read\n",
      "100 images read\n",
      "100 images read\n",
      "100 images read\n",
      "100 images read\n",
      "100 images read\n",
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# Train data directories\n",
    "base_train_dir = 'Tomato/Train'\n",
    "base_test_dir = 'Tomato/Val'\n",
    "\n",
    "\n",
    "\n",
    "# Assuming base_train_dir and base_test_dir variables are already defined\n",
    "# Train data directories\n",
    "bacterial_spot_train = read_images(os.path.join(base_train_dir, 'Bacterial_spot'))\n",
    "early_blight_train = read_images(os.path.join(base_train_dir, 'Early_blight'))\n",
    "healthy_train = read_images(os.path.join(base_train_dir, 'Healthy'))  # Included explicitly for the 'Healthy' class\n",
    "late_blight_train = read_images(os.path.join(base_train_dir, 'Late_blight'))\n",
    "leaf_mold_train = read_images(os.path.join(base_train_dir, 'Leaf_Mold'))\n",
    "mosaic_virus_train = read_images(os.path.join(base_train_dir, 'Mosaic_virus'))\n",
    "septoria_leaf_spot_train = read_images(os.path.join(base_train_dir, 'Septoria_leaf_spot'))\n",
    "spider_mites_train = read_images(os.path.join(base_train_dir, 'Spider_mites'))\n",
    "target_spot_train = read_images(os.path.join(base_train_dir, 'Target_Spot'))\n",
    "yellow_leaf_curl_virus_train = read_images(os.path.join(base_train_dir, 'Yellow_Leaf_Curl_Virus'))\n",
    "\n",
    "# Test data directories\n",
    "bacterial_spot_test = read_images(os.path.join(base_test_dir, 'Bacterial_spot'))\n",
    "early_blight_test = read_images(os.path.join(base_test_dir, 'Early_blight'))\n",
    "healthy_test = read_images(os.path.join(base_test_dir, 'Healthy'))  # Included explicitly for the 'Healthy' class\n",
    "late_blight_test = read_images(os.path.join(base_test_dir, 'Late_blight'))\n",
    "leaf_mold_test = read_images(os.path.join(base_test_dir, 'Leaf_Mold'))\n",
    "mosaic_virus_test = read_images(os.path.join(base_test_dir, 'Mosaic_virus'))\n",
    "septoria_leaf_spot_test = read_images(os.path.join(base_test_dir, 'Septoria_leaf_spot'))\n",
    "spider_mites_test = read_images(os.path.join(base_test_dir, 'Spider_mites'))\n",
    "target_spot_test = read_images(os.path.join(base_test_dir, 'Target_Spot'))\n",
    "yellow_leaf_curl_virus_test = read_images(os.path.join(base_test_dir, 'Yellow_Leaf_Curl_Virus'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hc04CAnpDnGK",
    "outputId": "2da9da6a-bcc3-4843-87df-9827c4ea2ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Bacterial_spot train: 1496\n",
      "Length of Early_blight train: 704\n",
      "Length of Healthy train: 1119\n",
      "Length of Late_blight train: 1343\n",
      "Length of Leaf_Mold train: 669\n",
      "Length of Mosaic_virus train: 262\n",
      "Length of Septoria_leaf_spot train: 1246\n",
      "Length of Spider_mites Two-spotted_spider_mite train: 1179\n",
      "Length of Target_Spot train: 988\n",
      "Length of Yellow_Leaf_Curl_Virus train: 3770\n",
      "Length of Bacterial_spot test: 205\n",
      "Length of Early_blight test: 96\n",
      "Length of Healthy test: 153\n",
      "Length of Late_blight test: 184\n",
      "Length of Leaf_Mold test: 92\n",
      "Length of Mosaic_virus test: 36\n",
      "Length of Septoria_leaf_spot test: 170\n",
      "Length of Spider_mites Two-spotted_spider_mite test: 161\n",
      "Length of Target_Spot test: 135\n",
      "Length of Yellow_Leaf_Curl_Virus test: 515\n",
      "Total length of all train parts: 12776\n",
      "Total length of all test parts: 1747\n",
      "Total length of all parts: 14523\n"
     ]
    }
   ],
   "source": [
    "# For training classes\n",
    "print(\"Length of Bacterial_spot train:\", len(bacterial_spot_train))\n",
    "print(\"Length of Early_blight train:\", len(early_blight_train))\n",
    "print(\"Length of Healthy train:\", len(healthy_train))\n",
    "print(\"Length of Late_blight train:\", len(late_blight_train))\n",
    "print(\"Length of Leaf_Mold train:\", len(leaf_mold_train))\n",
    "print(\"Length of Mosaic_virus train:\", len(mosaic_virus_train))\n",
    "print(\"Length of Septoria_leaf_spot train:\", len(septoria_leaf_spot_train))\n",
    "print(\"Length of Spider_mites Two-spotted_spider_mite train:\", len(spider_mites_train))\n",
    "print(\"Length of Target_Spot train:\", len(target_spot_train))\n",
    "print(\"Length of Yellow_Leaf_Curl_Virus train:\", len(yellow_leaf_curl_virus_train))\n",
    "\n",
    "# For test classes\n",
    "print(\"Length of Bacterial_spot test:\", len(bacterial_spot_test))\n",
    "print(\"Length of Early_blight test:\", len(early_blight_test))\n",
    "print(\"Length of Healthy test:\", len(healthy_test))\n",
    "print(\"Length of Late_blight test:\", len(late_blight_test))\n",
    "print(\"Length of Leaf_Mold test:\", len(leaf_mold_test))\n",
    "print(\"Length of Mosaic_virus test:\", len(mosaic_virus_test))\n",
    "print(\"Length of Septoria_leaf_spot test:\", len(septoria_leaf_spot_test))\n",
    "print(\"Length of Spider_mites Two-spotted_spider_mite test:\", len(spider_mites_test))\n",
    "print(\"Length of Target_Spot test:\", len(target_spot_test))\n",
    "print(\"Length of Yellow_Leaf_Curl_Virus test:\", len(yellow_leaf_curl_virus_test))\n",
    "\n",
    "# Calculate the lengths of all train parts\n",
    "train_lengths = len(bacterial_spot_train) + len(early_blight_train) + len(healthy_train) + \\\n",
    "                len(late_blight_train) + len(leaf_mold_train) + len(mosaic_virus_train) + \\\n",
    "                len(septoria_leaf_spot_train) + len(spider_mites_train) + \\\n",
    "                len(target_spot_train) + len(yellow_leaf_curl_virus_train)\n",
    "\n",
    "# Calculate the lengths of all test parts\n",
    "test_lengths = len(bacterial_spot_test) + len(early_blight_test) + len(healthy_test) + \\\n",
    "               len(late_blight_test) + len(leaf_mold_test) + len(mosaic_virus_test) + \\\n",
    "               len(septoria_leaf_spot_test) + len(spider_mites_test) + \\\n",
    "               len(target_spot_test) + len(yellow_leaf_curl_virus_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Total length of all train parts:\", train_lengths)\n",
    "print(\"Total length of all test parts:\", test_lengths)\n",
    "\n",
    "# Calculate the total length of all parts\n",
    "total_length = train_lengths + test_lengths\n",
    "\n",
    "# Print the results\n",
    "print(\"Total length of all parts:\", total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8GPaaxQYDqQM"
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def add_gaussian_noise(images, mean_range=(0, 15), std_range=(0, 0.15)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.AdditiveGaussianNoise(loc=mean_range, scale=(0, 0.2*255))\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def random_crop(images, crop_percent=(0.1, 0.4)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Crop(percent=crop_percent)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def random_rotate(images, rotation_range=(-360, 360)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Rotate(rotate=rotation_range)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def invert_images(images):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Add(value=(-20, 20))\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "def adjust_brightness(images, brightness_range=(-65, 65)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for adjusting brightness\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Multiply((1.0 + brightness_range[0] / 100.0, 1.0 + brightness_range[1] / 100.0))\n",
    "    ])\n",
    "\n",
    "\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "def scale_images(images, scale_factor = (0.3, 1.8)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for scaling images\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Affine(scale=scale_factor)\n",
    "    ])\n",
    "\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def add_contrast(images, contrast_factor=(0.5, 1.5)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for adding contrast\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.ContrastNormalization(alpha=contrast_factor)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images_np]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def flip_images(images, flip_probability=0.5):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for randomly flipping images\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Sometimes(flip_probability, iaa.Fliplr(1.0)),  # Horizontal flips\n",
    "        iaa.Sometimes(flip_probability, iaa.Flipud(1.0))   # Vertical flips\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images_np]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "# discard images according to ratio\n",
    "def discard_images(images, discard_ratio=0.5):\n",
    "    random.seed(10)\n",
    "    # Calculate the number of images to discard based on the discard_ratio\n",
    "    num_images_to_discard = int(len(images) * discard_ratio)\n",
    "\n",
    "    # Create a copy of the input list to avoid modifying the original list\n",
    "    remaining_images = images[:]\n",
    "\n",
    "    # Randomly discard a portion of the images\n",
    "    random.shuffle(remaining_images)\n",
    "    remaining_images = remaining_images[num_images_to_discard:]\n",
    "\n",
    "    return remaining_images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resize_images(images_list, width=128, height=128):\n",
    "    ia.seed(1)\n",
    "    # Define the resize augmentation\n",
    "    resize_augmenter = iaa.Resize({\"height\": height, \"width\": width})\n",
    "\n",
    "    resized_images = []\n",
    "\n",
    "    for image in images_list:\n",
    "        # Ensure the image is in RGB format (imgaug uses RGB by default)\n",
    "        if image.shape[-1] == 1:  # Grayscale image with single channel\n",
    "            image = np.repeat(image, 3, axis=-1)\n",
    "\n",
    "        # Apply the resize augmentation\n",
    "        augmented_image = resize_augmenter.augment_image(image)\n",
    "\n",
    "        # Append the augmented image to the result list\n",
    "        resized_images.append(augmented_image)\n",
    "\n",
    "    del images_list[:]\n",
    "    return resized_images\n",
    "\n",
    "def keep_n_images(images, n_to_keep):\n",
    "    random.seed(10)\n",
    "    if n_to_keep >= len(images):\n",
    "        return images  # Keep all images if n_to_keep is greater than or equal to the image count\n",
    "\n",
    "    # Randomly shuffle the images list\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Keep the first n_to_keep images and discard the rest\n",
    "    kept_images = images[:n_to_keep]\n",
    "\n",
    "    # Create a copy of the kept images list\n",
    "    kept_images_copy = copy.deepcopy(kept_images)\n",
    "\n",
    "    # Clear the original images list to free memory\n",
    "    del images[:]\n",
    "\n",
    "    return kept_images_copy\n",
    "import cv2\n",
    "def normalize_images(image_list):\n",
    "\n",
    "  for i in range(len(image_list)):\n",
    "      image = image_list[i].astype(np.float32) / 255.0\n",
    "      image_list[i] = image\n",
    "\n",
    "def discard_images(lst, percent_to_discard):\n",
    "    # Calculate the number of images to discard\n",
    "    num_images_to_discard = int(len(lst) * percent_to_discard)\n",
    "    \n",
    "    # Generate a list of unique indices to discard\n",
    "    indices_to_discard = random.sample(range(len(lst)), num_images_to_discard)\n",
    "    \n",
    "    # For each index to discard, if there's a need for explicit clean-up (e.g., closing a file or freeing resources), do it here\n",
    "    for index in indices_to_discard:\n",
    "        image = lst[index]\n",
    "        # If the image is an object that requires closing or special disposal, do it here\n",
    "        # For example, if image objects are from PIL, you might need to call image.close() if they are open files\n",
    "        # Note: This is only necessary if your image objects require explicit clean-up\n",
    "    \n",
    "    # Create a new list excluding the indices to discard\n",
    "    modified_lst = [lst[i] for i in range(len(lst)) if i not in indices_to_discard]\n",
    "    \n",
    "    return modified_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTks90BUDq62",
    "outputId": "b73b85b4-ff77-49ad-f68b-17a80d7564be"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# [2394, 1127, 1791, 2149, 1071, 420, 1994, 1887, 1581, 6032]\n",
    "\n",
    "# \"\"\"\n",
    "# Length of Bacterial_spot train: 1496\n",
    "# Length of Early_blight train: 704\n",
    "# Length of Healthy train: 1119\n",
    "# Length of Late_blight train: 1343\n",
    "# Length of Leaf_Mold train: 669\n",
    "# Length of Mosaic_virus train: 262\n",
    "# Length of Septoria_leaf_spot train: 1246\n",
    "# Length of Spider_mites Two-spotted_spider_mite train: 1179\n",
    "# Length of Target_Spot train: 988\n",
    "# Length of Yellow_Leaf_Curl_Virus train: 3770\n",
    "# \"\"\"\n",
    "\n",
    "# # bacterial_spot_all = (\n",
    "# #     add_gaussian_noise(bacterial_spot_train) +\n",
    "# #     random_crop(bacterial_spot_train) +\n",
    "# #     invert_images(bacterial_spot_train) +\n",
    "# #     adjust_brightness(bacterial_spot_train) +\n",
    "# #     scale_images(bacterial_spot_train) +\n",
    "# #     random_rotate(bacterial_spot_train)\n",
    "# # )\n",
    "# bacterial_spot_all = []\n",
    "\n",
    "# early_blight_all = (\n",
    "#     add_gaussian_noise(early_blight_train) +\n",
    "#     random_crop(early_blight_train) +\n",
    "#     invert_images(early_blight_train) +\n",
    "#     adjust_brightness(early_blight_train) +\n",
    "#     scale_images(early_blight_train) +\n",
    "#     random_rotate(early_blight_train)\n",
    "# )\n",
    "\n",
    "# healthy_all = (\n",
    "#     add_gaussian_noise(healthy_train) +\n",
    "#     random_crop(healthy_train) +\n",
    "#     invert_images(healthy_train) +\n",
    "#     adjust_brightness(healthy_train) +\n",
    "#     scale_images(healthy_train) +\n",
    "#     random_rotate(healthy_train)\n",
    "# )\n",
    "\n",
    "# # late_blight_all = (\n",
    "# #     add_gaussian_noise(late_blight_train) +\n",
    "# #     random_crop(late_blight_train) +\n",
    "# #     invert_images(late_blight_train) +\n",
    "# #     adjust_brightness(late_blight_train) +\n",
    "# #     scale_images(late_blight_train) +\n",
    "# #     random_rotate(late_blight_train)\n",
    "# # )\n",
    "# late_blight_all = []\n",
    "\n",
    "# leaf_mold_all = (\n",
    "#     add_gaussian_noise(leaf_mold_train) +\n",
    "#     random_crop(leaf_mold_train) +\n",
    "#     invert_images(leaf_mold_train) +\n",
    "#     adjust_brightness(leaf_mold_train) +\n",
    "#     scale_images(leaf_mold_train) +\n",
    "#     random_rotate(leaf_mold_train)\n",
    "# )\n",
    "\n",
    "# mosaic_virus_all = (\n",
    "#     add_gaussian_noise(mosaic_virus_train) +\n",
    "#     random_crop(mosaic_virus_train) +\n",
    "#     invert_images(mosaic_virus_train) +\n",
    "#     adjust_brightness(mosaic_virus_train) +\n",
    "#     scale_images(mosaic_virus_train) +\n",
    "#     random_rotate(mosaic_virus_train)\n",
    "# )\n",
    "\n",
    "# septoria_leaf_spot_all = (\n",
    "#     add_gaussian_noise(septoria_leaf_spot_train) +\n",
    "#     random_crop(septoria_leaf_spot_train) +\n",
    "#     invert_images(septoria_leaf_spot_train) +\n",
    "#     adjust_brightness(septoria_leaf_spot_train) +\n",
    "#     scale_images(septoria_leaf_spot_train) +\n",
    "#     random_rotate(septoria_leaf_spot_train)\n",
    "# )\n",
    "\n",
    "# spider_mites_all = (\n",
    "#     add_gaussian_noise(spider_mites_train) +\n",
    "#     random_crop(spider_mites_train) +\n",
    "#     invert_images(spider_mites_train) +\n",
    "#     adjust_brightness(spider_mites_train) +\n",
    "#     scale_images(spider_mites_train) +\n",
    "#     random_rotate(spider_mites_train)\n",
    "# )\n",
    "\n",
    "# target_spot_all = (\n",
    "#     add_gaussian_noise(target_spot_train) +\n",
    "#     random_crop(target_spot_train) +\n",
    "#     invert_images(target_spot_train) +\n",
    "#     adjust_brightness(target_spot_train) +\n",
    "#     scale_images(target_spot_train) +\n",
    "#     random_rotate(target_spot_train)\n",
    "# )\n",
    "\n",
    "# # yellow_leaf_curl_virus_all = (\n",
    "# #     add_gaussian_noise(yellow_leaf_curl_virus_train) +\n",
    "# #     random_crop(yellow_leaf_curl_virus_train) +\n",
    "# #     invert_images(yellow_leaf_curl_virus_train) +\n",
    "# #     adjust_brightness(yellow_leaf_curl_virus_train) +\n",
    "# #     scale_images(yellow_leaf_curl_virus_train) +\n",
    "# #     random_rotate(yellow_leaf_curl_virus_train)\n",
    "# # )\n",
    "\n",
    "# yellow_leaf_curl_virus_all = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Correcting the discard percentage to match the description\n",
    "# # discard_percentage = 0.9\n",
    "\n",
    "# # Overwrite each class variable with the reduced dataset after discarding 52% of images\n",
    "# # bacterial_spot_all = discard_images(bacterial_spot_all, 1)\n",
    "# early_blight_all = discard_images(early_blight_all, 0.81)\n",
    "# healthy_all = discard_images(healthy_all, 0.93) \n",
    "# # late_blight_all = discard_images(late_blight_all, 1)\n",
    "# leaf_mold_all = discard_images(leaf_mold_all, 0.8)\n",
    "# mosaic_virus_all = discard_images(mosaic_virus_all, 0.16)\n",
    "# septoria_leaf_spot_all = discard_images(septoria_leaf_spot_all, 0.96)\n",
    "# spider_mites_all = discard_images(spider_mites_all, 0.96)\n",
    "# target_spot_all = discard_images(target_spot_all, 0.91)\n",
    "# # yellow_leaf_curl_virus_all = discard_images(yellow_leaf_curl_virus_all, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HaxJ-J5qk8QI"
   },
   "outputs": [],
   "source": [
    "# bacterial_spot_train = bacterial_spot_train + bacterial_spot_all\n",
    "# early_blight_train = early_blight_train + early_blight_all\n",
    "# healthy_train = healthy_train + healthy_all  # Explicitly included for the 'Healthy' class\n",
    "# late_blight_train = late_blight_train + late_blight_all\n",
    "# leaf_mold_train = leaf_mold_train + leaf_mold_all\n",
    "# mosaic_virus_train = mosaic_virus_train + mosaic_virus_all\n",
    "# septoria_leaf_spot_train = septoria_leaf_spot_train + septoria_leaf_spot_all\n",
    "# spider_mites_train = spider_mites_train + spider_mites_all\n",
    "# target_spot_train = target_spot_train + target_spot_all\n",
    "# yellow_leaf_curl_virus_train = yellow_leaf_curl_virus_train + yellow_leaf_curl_virus_all\n",
    "\n",
    "# yellow_leaf_curl_virus_train = discard_images(yellow_leaf_curl_virus_train, 0.6)\n",
    "\n",
    "\n",
    "# Define the percentage of images to discard\n",
    "discard_percentage = 0.7  # This means 50% of the data will be discarded\n",
    "\n",
    "# Apply the discard_images function to each dataset with the defined discard percentage\n",
    "bacterial_spot_train = discard_images(bacterial_spot_train, discard_percentage)\n",
    "early_blight_train = discard_images(early_blight_train, discard_percentage)\n",
    "healthy_train = discard_images(healthy_train, discard_percentage)\n",
    "late_blight_train = discard_images(late_blight_train, discard_percentage)\n",
    "leaf_mold_train = discard_images(leaf_mold_train, discard_percentage)\n",
    "mosaic_virus_train = discard_images(mosaic_virus_train, discard_percentage)\n",
    "septoria_leaf_spot_train = discard_images(septoria_leaf_spot_train, discard_percentage)\n",
    "spider_mites_train = discard_images(spider_mites_train, discard_percentage)\n",
    "target_spot_train = discard_images(target_spot_train, discard_percentage)\n",
    "yellow_leaf_curl_virus_train = discard_images(yellow_leaf_curl_virus_train, discard_percentage)\n",
    "\n",
    "# After executing the above lines, each dataset variable will now reference a subset\n",
    "# that retains only 50% of the original images, since we're\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pBIUOdVHD1pD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate labels for the new training datasets\n",
    "labels_bacterial_spot_train = np.zeros(len(bacterial_spot_train))\n",
    "labels_early_blight_train = np.ones(len(early_blight_train))\n",
    "labels_healthy_train = np.full(len(healthy_train), 2)  # Adjusted label for 'Healthy'\n",
    "labels_late_blight_train = np.full(len(late_blight_train), 3)\n",
    "labels_leaf_mold_train = np.full(len(leaf_mold_train), 4)\n",
    "labels_mosaic_virus_train = np.full(len(mosaic_virus_train), 5)\n",
    "labels_septoria_leaf_spot_train = np.full(len(septoria_leaf_spot_train), 6)\n",
    "labels_spider_mites_train = np.full(len(spider_mites_train), 7)\n",
    "labels_target_spot_train = np.full(len(target_spot_train), 8)\n",
    "labels_yellow_leaf_curl_virus_train = np.full(len(yellow_leaf_curl_virus_train), 9)\n",
    "\n",
    "# Combine the new training labels\n",
    "labels_train = np.concatenate([\n",
    "    labels_bacterial_spot_train,\n",
    "    labels_early_blight_train,\n",
    "    labels_healthy_train,\n",
    "    labels_late_blight_train,\n",
    "    labels_leaf_mold_train,\n",
    "    labels_mosaic_virus_train,\n",
    "    labels_septoria_leaf_spot_train,\n",
    "    labels_spider_mites_train,\n",
    "    labels_target_spot_train,\n",
    "    labels_yellow_leaf_curl_virus_train\n",
    "])\n",
    "\n",
    "# Generate labels for the new test datasets\n",
    "labels_bacterial_spot_test = np.zeros(len(bacterial_spot_test))\n",
    "labels_early_blight_test = np.ones(len(early_blight_test))\n",
    "labels_healthy_test = np.full(len(healthy_test), 2)  # Adjusted label for 'Healthy'\n",
    "labels_late_blight_test = np.full(len(late_blight_test), 3)\n",
    "labels_leaf_mold_test = np.full(len(leaf_mold_test), 4)\n",
    "labels_mosaic_virus_test = np.full(len(mosaic_virus_test), 5)\n",
    "labels_septoria_leaf_spot_test = np.full(len(septoria_leaf_spot_test), 6)\n",
    "labels_spider_mites_test = np.full(len(spider_mites_test), 7)\n",
    "labels_target_spot_test = np.full(len(target_spot_test), 8)\n",
    "labels_yellow_leaf_curl_virus_test = np.full(len(yellow_leaf_curl_virus_test), 9)\n",
    "\n",
    "# Combine the new test labels\n",
    "labels_test = np.concatenate([\n",
    "    labels_bacterial_spot_test,\n",
    "    labels_early_blight_test,\n",
    "    labels_healthy_test,\n",
    "    labels_late_blight_test,\n",
    "    labels_leaf_mold_test,\n",
    "    labels_mosaic_virus_test,\n",
    "    labels_septoria_leaf_spot_test,\n",
    "    labels_spider_mites_test,\n",
    "    labels_target_spot_test,\n",
    "    labels_yellow_leaf_curl_virus_test\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OBt4R0viD3Pz"
   },
   "outputs": [],
   "source": [
    "dim = 224\n",
    "\n",
    "# Resize each group of images to dimxdim pixels separately for the new classes\n",
    "bacterial_spot_train_resized = resize_images(bacterial_spot_train, dim, dim)\n",
    "early_blight_train_resized = resize_images(early_blight_train, dim, dim)\n",
    "healthy_train_resized = resize_images(healthy_train, dim, dim)  # Included explicitly for the 'Healthy' class\n",
    "late_blight_train_resized = resize_images(late_blight_train, dim, dim)\n",
    "leaf_mold_train_resized = resize_images(leaf_mold_train, dim, dim)\n",
    "mosaic_virus_train_resized = resize_images(mosaic_virus_train, dim, dim)\n",
    "septoria_leaf_spot_train_resized = resize_images(septoria_leaf_spot_train, dim, dim)\n",
    "spider_mites_train_resized = resize_images(spider_mites_train, dim, dim)\n",
    "target_spot_train_resized = resize_images(target_spot_train, dim, dim)\n",
    "yellow_leaf_curl_virus_train_resized = resize_images(yellow_leaf_curl_virus_train, dim, dim)\n",
    "\n",
    "# Resize each group of images for the test datasets\n",
    "bacterial_spot_test_resized = resize_images(bacterial_spot_test, dim, dim)\n",
    "early_blight_test_resized = resize_images(early_blight_test, dim, dim)\n",
    "healthy_test_resized = resize_images(healthy_test, dim, dim)  # Included explicitly for the 'Healthy' class\n",
    "late_blight_test_resized = resize_images(late_blight_test, dim, dim)\n",
    "leaf_mold_test_resized = resize_images(leaf_mold_test, dim, dim)\n",
    "mosaic_virus_test_resized = resize_images(mosaic_virus_test, dim, dim)\n",
    "septoria_leaf_spot_test_resized = resize_images(septoria_leaf_spot_test, dim, dim)\n",
    "spider_mites_test_resized = resize_images(spider_mites_test, dim, dim)\n",
    "target_spot_test_resized = resize_images(target_spot_test, dim, dim)\n",
    "yellow_leaf_curl_virus_test_resized = resize_images(yellow_leaf_curl_virus_test, dim, dim)\n",
    "\n",
    "# Concatenate the resized images for the training and testing datasets\n",
    "images_train = bacterial_spot_train_resized + early_blight_train_resized + healthy_train_resized + \\\n",
    "               late_blight_train_resized + leaf_mold_train_resized + mosaic_virus_train_resized + \\\n",
    "               septoria_leaf_spot_train_resized + spider_mites_train_resized + \\\n",
    "               target_spot_train_resized + yellow_leaf_curl_virus_train_resized\n",
    "\n",
    "images_test = bacterial_spot_test_resized + early_blight_test_resized + healthy_test_resized + \\\n",
    "              late_blight_test_resized + leaf_mold_test_resized + mosaic_virus_test_resized + \\\n",
    "              septoria_leaf_spot_test_resized + spider_mites_test_resized + \\\n",
    "              target_spot_test_resized + yellow_leaf_curl_virus_test_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[119, 115, 114],\n",
       "        [111, 107, 106],\n",
       "        [120, 116, 115],\n",
       "        ...,\n",
       "        [116, 112, 111],\n",
       "        [122, 118, 117],\n",
       "        [125, 121, 120]],\n",
       "\n",
       "       [[113, 109, 108],\n",
       "        [109, 105, 104],\n",
       "        [124, 120, 119],\n",
       "        ...,\n",
       "        [123, 119, 118],\n",
       "        [130, 126, 125],\n",
       "        [131, 127, 126]],\n",
       "\n",
       "       [[123, 119, 118],\n",
       "        [113, 109, 108],\n",
       "        [118, 114, 113],\n",
       "        ...,\n",
       "        [122, 118, 117],\n",
       "        [127, 123, 122],\n",
       "        [129, 125, 124]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[150, 148, 149],\n",
       "        [151, 149, 150],\n",
       "        [150, 148, 149],\n",
       "        ...,\n",
       "        [160, 158, 159],\n",
       "        [159, 157, 158],\n",
       "        [159, 157, 158]],\n",
       "\n",
       "       [[147, 145, 146],\n",
       "        [149, 147, 148],\n",
       "        [151, 149, 150],\n",
       "        ...,\n",
       "        [156, 154, 155],\n",
       "        [154, 152, 153],\n",
       "        [153, 151, 152]],\n",
       "\n",
       "       [[143, 141, 142],\n",
       "        [147, 145, 146],\n",
       "        [151, 149, 150],\n",
       "        ...,\n",
       "        [159, 157, 158],\n",
       "        [157, 155, 156],\n",
       "        [157, 155, 156]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fm18BEYAxqG9"
   },
   "outputs": [],
   "source": [
    "\n",
    "normalize_images(images_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzUKhudfCMuR",
    "outputId": "3c35d115-5756-4376-f26e-366e44aee695"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5254902 , 0.5176471 , 0.6039216 ],\n",
       "        [0.43137255, 0.42352942, 0.50980395],\n",
       "        [0.42352942, 0.41568628, 0.5019608 ],\n",
       "        ...,\n",
       "        [0.44313726, 0.43137255, 0.5137255 ],\n",
       "        [0.6156863 , 0.6039216 , 0.6862745 ],\n",
       "        [0.45882353, 0.44705883, 0.5294118 ]],\n",
       "\n",
       "       [[0.5137255 , 0.5058824 , 0.5921569 ],\n",
       "        [0.44313726, 0.43529412, 0.52156866],\n",
       "        [0.57254905, 0.5647059 , 0.6509804 ],\n",
       "        ...,\n",
       "        [0.44313726, 0.43137255, 0.5137255 ],\n",
       "        [0.49019608, 0.47843137, 0.56078434],\n",
       "        [0.36078432, 0.34901962, 0.43137255]],\n",
       "\n",
       "       [[0.3764706 , 0.36862746, 0.45490196],\n",
       "        [0.5294118 , 0.52156866, 0.60784316],\n",
       "        [0.40392157, 0.39607844, 0.48235294],\n",
       "        ...,\n",
       "        [0.32941177, 0.31764707, 0.4       ],\n",
       "        [0.58431375, 0.57254905, 0.654902  ],\n",
       "        [0.45490196, 0.44313726, 0.5254902 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.67058825, 0.654902  , 0.72156864],\n",
       "        [0.39215687, 0.3764706 , 0.44313726],\n",
       "        [0.5686275 , 0.5529412 , 0.61960787],\n",
       "        ...,\n",
       "        [0.42352942, 0.39607844, 0.45882353],\n",
       "        [0.30980393, 0.28235295, 0.34509805],\n",
       "        [0.4627451 , 0.43529412, 0.49803922]],\n",
       "\n",
       "       [[0.3882353 , 0.37254903, 0.4392157 ],\n",
       "        [0.5254902 , 0.50980395, 0.5764706 ],\n",
       "        [0.5686275 , 0.5529412 , 0.61960787],\n",
       "        ...,\n",
       "        [0.3882353 , 0.36078432, 0.42352942],\n",
       "        [0.43137255, 0.40392157, 0.46666667],\n",
       "        [0.34509805, 0.31764707, 0.38039216]],\n",
       "\n",
       "       [[0.5686275 , 0.5529412 , 0.61960787],\n",
       "        [0.4745098 , 0.45882353, 0.5254902 ],\n",
       "        [0.53333336, 0.5176471 , 0.58431375],\n",
       "        ...,\n",
       "        [0.3764706 , 0.34901962, 0.4117647 ],\n",
       "        [0.4862745 , 0.45882353, 0.52156866],\n",
       "        [0.5058824 , 0.47843137, 0.5411765 ]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gVOutQGDxwfT"
   },
   "outputs": [],
   "source": [
    "normalize_images(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vKXb38zpD6tC"
   },
   "outputs": [],
   "source": [
    "#TEST Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X125FDBD9Pd",
    "outputId": "b7d3f1ef-a11b-4c69-b687-dc8c6dc67785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3836\n",
      "3836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[449, 212, 336, 403, 201, 79, 374, 354, 297, 1131]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(images_train))\n",
    "print(len(labels_train))\n",
    "\n",
    "counts = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for label in labels_train:\n",
    "    counts[int(label)] += 1\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SS_dvGp1D_zT"
   },
   "outputs": [],
   "source": [
    "\n",
    "ls1 = images_test\n",
    "ls2 = images_train\n",
    "\n",
    "images_test = np.array(images_test)\n",
    "images_train = np.array(images_train)\n",
    "\n",
    "del ls1\n",
    "del ls2\n",
    "\n",
    "\n",
    "shuffle_indices_train = np.random.permutation(len(images_train))\n",
    "shuffle_indices_test = np.random.permutation(len(images_test))\n",
    "\n",
    "\n",
    "images_train = images_train[shuffle_indices_train]\n",
    "labels_train = labels_train[shuffle_indices_train]\n",
    "\n",
    "images_test = images_test[shuffle_indices_test]\n",
    "labels_test = labels_test[shuffle_indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9NsYhuwREBCD"
   },
   "outputs": [],
   "source": [
    "# %load custom_callback.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Find accuracy of model\n",
    "def find_accuracy(test,pred):\n",
    "    correct = 0\n",
    "    total = len(test)\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        if test[i] == pred[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "# Map ANN outputs to classes\n",
    "def get_labels(y_pred_ann):\n",
    "    labels = []\n",
    "\n",
    "    for pred in y_pred_ann:\n",
    "        max_index = 0\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] > pred[max_index]:\n",
    "                max_index = i\n",
    "\n",
    "        labels.append(max_index)\n",
    "\n",
    "    return labels\n",
    "\n",
    "# This callback prints accuracy by epoch information after each epoch\n",
    "class Save_Accuracy_By_Epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.X_Test = test_data[0]\n",
    "        self.Y_Test = test_data[1]\n",
    "        self.accuracies = []\n",
    "        self.epochs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        y_pred = self.model.predict(self.X_Test)\n",
    "\n",
    "        if epoch == 4:\n",
    "            pass\n",
    "\n",
    "        y_pred = get_labels(y_pred)\n",
    "        accuracy = find_accuracy(self.Y_Test, y_pred)\n",
    "        self.epochs.append(epoch+1)\n",
    "        self.accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        print(self.epochs)\n",
    "        print(self.accuracies)\n",
    "\n",
    "\n",
    "# This callback prints metrics for every class after each epoch\n",
    "class Save_Multiclass_Metrics_By_Epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data, n_classes, save_after = 10,save_thresh =0.94, save_csv_path = 'results.csv', model_name = 'model.keras', decay_rate = 0.9, min_rate = 0.000032, save_folder_name = \".\"):\n",
    "        self.X_Test = test_data[0]\n",
    "        self.Y_Test = test_data[1]\n",
    "\n",
    "        self.min_rate = min_rate;\n",
    "        \n",
    "        self.epochs = []\n",
    "        self.n_classes = n_classes\n",
    "        self.save_after = save_after\n",
    "        self.save_csv_path = save_csv_path\n",
    "        self.save_folder_name = save_folder_name\n",
    "        self.model_name = model_name\n",
    "        self.max_accuracy = 0\n",
    "        self.save_thresh = save_thresh\n",
    "\n",
    "        self.mat_sensitivity = []\n",
    "        self.mat_specificity = []\n",
    "        self.mat_precision = []\n",
    "        self.mat_recall = []\n",
    "        self.mat_accuracy = []\n",
    "        self.mat_f1 = []\n",
    "        self.accuracies = []\n",
    "\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.mat_sensitivity.append([])\n",
    "            self.mat_specificity.append([])\n",
    "            self.mat_precision.append([])\n",
    "            self.mat_recall.append([])\n",
    "            self.mat_accuracy.append([])\n",
    "            self.mat_f1.append([])\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        y_pred = self.model.predict(self.X_Test)\n",
    "        y_pred = get_labels(y_pred)\n",
    "\n",
    "        total = len(self.Y_Test)\n",
    "\n",
    "        correct = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == self.Y_Test[i]:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct/total\n",
    "        self.accuracies.append(correct / total)\n",
    "\n",
    "        best = False\n",
    "\n",
    "        if accuracy >= self.max_accuracy:\n",
    "            self.max_accuracy = accuracy\n",
    "            best = True\n",
    "\n",
    "\n",
    "        if accuracy > self.save_thresh:\n",
    "          best = True\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "            FN = 0\n",
    "\n",
    "            for j in range(len(y_pred)):\n",
    "                if self.Y_Test[j] == i and y_pred[j] == i:\n",
    "                    TP += 1\n",
    "                elif self.Y_Test[j] != i and y_pred[j] == i:\n",
    "                    FP += 1\n",
    "                elif self.Y_Test[j] == i and y_pred[j] != i:\n",
    "                    FN += 1\n",
    "                elif self.Y_Test[j] != i and y_pred[j] != i:\n",
    "                    TN += 1\n",
    "\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else -1\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else -1\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
    "            accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else -1\n",
    "\n",
    "            self.mat_sensitivity[i].append(sensitivity)\n",
    "            self.mat_specificity[i].append(specificity)\n",
    "            self.mat_precision[i].append(precision)\n",
    "            self.mat_recall[i].append(recall)\n",
    "            self.mat_accuracy[i].append(accuracy)\n",
    "            self.mat_f1[i].append(f1)\n",
    "\n",
    "        self.epochs.append(int(epoch+1))\n",
    "\n",
    "        print(\"max Accuracy: \", self.max_accuracy, \"Learning Rate: \",self.model.optimizer.learning_rate.numpy())\n",
    "           \n",
    "\n",
    "        if self.model.optimizer.learning_rate <= self.min_rate :\n",
    "           self.model.stop_training = True\n",
    "\n",
    "        if (epoch + 1) % self.save_after == 0:\n",
    "            save_to_csv_file(self.save_csv_path, self.mat_sensitivity, self.mat_specificity, self.mat_precision, self.mat_recall, self.mat_accuracy, self.mat_f1, self.accuracies, self.epochs)\n",
    "            self.model.save(self.model_name)\n",
    "            if best:\n",
    "                self.model.save(self.save_folder_name + \"/epoch\" + str((epoch+1)) + \"_\" + self.model_name)\n",
    "            pass\n",
    "        \n",
    "\n",
    " \n",
    "\n",
    "def save_to_csv_file(path, mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1, accuracies, epochs):\n",
    "    mat_sensitivity = np.transpose(mat_sensitivity)\n",
    "    mat_specificity = np.transpose(mat_specificity)\n",
    "    mat_precision = np.transpose(mat_precision)\n",
    "    mat_recall = np.transpose(mat_recall)\n",
    "    mat_accuracy = np.transpose(mat_accuracy)\n",
    "    mat_f1 = np.transpose(mat_f1)\n",
    "    accuracies = np.reshape(accuracies, (-1,1))\n",
    "    epochs = np.reshape(epochs, (-1,1))\n",
    "\n",
    "    mat_join = np.concatenate((epochs,accuracies,mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1), axis = 1)\n",
    "\n",
    "    n = mat_sensitivity.shape[1]\n",
    "\n",
    "\n",
    "    col_array_sensitivity = list(range(n))\n",
    "    col_array_specificity = list(range(n))\n",
    "    col_array_precision = list(range(n))\n",
    "    col_array_recall = list(range(n))\n",
    "    col_array_accuracy = list(range(n))\n",
    "    col_array_f1 = list(range(n))\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        col_array_sensitivity[i] = 'sensitivity Class ' + str(col_array_sensitivity[i])\n",
    "        col_array_specificity[i] = 'specificity Class ' + str(col_array_specificity[i])\n",
    "        col_array_precision[i] = 'precision Class ' + str(col_array_precision[i])\n",
    "        col_array_recall[i] = 'recall Class ' + str(col_array_recall[i])\n",
    "        col_array_accuracy[i] = 'accuracy Class ' + str(col_array_accuracy[i])\n",
    "        col_array_f1[i] = 'f1 Class ' + str(col_array_f1[i])\n",
    "\n",
    "    cols = ['Epoch']+['overall_accuracy']+ col_array_sensitivity + col_array_specificity + col_array_precision + col_array_recall + col_array_accuracy + col_array_f1\n",
    "\n",
    "    mat_join = np.flip(mat_join, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns = cols,\n",
    "        data  = mat_join\n",
    "    )\n",
    "\n",
    "    df.to_csv(path, index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6ewhCiGnECZl"
   },
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "labels_train_encoded = []\n",
    "for label in labels_train:\n",
    "    encoding = [0,0,0,0,0,0,0,0,0,0]\n",
    "    encoding[int(label)] = 1\n",
    "    labels_train_encoded.append(encoding)\n",
    "\n",
    "labels_train_encoded = np.array(labels_train_encoded)\n",
    "\n",
    "labels_test_encoded = []\n",
    "for label in labels_test:\n",
    "    encoding = [0,0,0,0,0,0,0,0,0,0]\n",
    "    encoding[int(label)] = 1\n",
    "    labels_test_encoded.append(encoding)\n",
    "\n",
    "labels_test_encoded = np.array(labels_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Lac-X6wtot2p"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "raw_model = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "\n",
    "model.add(raw_model)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D65LqJMLpa75",
    "outputId": "822f0c2e-7870-4865-f727-56d5c38321f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 07:24:19.640040: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 64)                167520    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 219626 (857.91 KB)\n",
      "Trainable params: 217458 (849.45 KB)\n",
      "Non-trainable params: 2168 (8.47 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.0003,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "tf.keras.utils.set_random_seed(2)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss=loss, metrics = ['acc'])\n",
    "# model.compile(optimizer='adam', loss=loss, metrics = ['acc'], loss_weights=class_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYya-q-9BNKB",
    "outputId": "f94d2482-ffac-4d00-ece2-a39d7a64bf2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 07:24:27.576151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2024-03-24 07:24:28.272082: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-24 07:24:31.877180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c23a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-24 07:24:31.877211: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-03-24 07:24:31.882127: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-24 07:24:32.006282: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - ETA: 0s - loss: 1.8724 - acc: 0.3757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 07:25:05.263278: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.19GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-24 07:25:05.263358: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-03-24 07:25:05.744446: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 5s 64ms/step\n",
      "max Accuracy:  0.3703491700057241 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 48s 137ms/step - loss: 1.8724 - acc: 0.3757 - val_loss: 1.9274 - val_acc: 0.3703\n",
      "Epoch 2/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.4979965655409273 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 104ms/step - loss: 1.4530 - acc: 0.5177 - val_loss: 1.7324 - val_acc: 0.4980\n",
      "Epoch 3/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.6451058958214081 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 1.2274 - acc: 0.5933 - val_loss: 1.0229 - val_acc: 0.6451\n",
      "Epoch 4/1000\n",
      "55/55 [==============================] - 3s 47ms/step\n",
      "max Accuracy:  0.705208929593589 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 1.1452 - acc: 0.6267 - val_loss: 0.9015 - val_acc: 0.7052\n",
      "Epoch 5/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.7653119633657699 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 1.0216 - acc: 0.6575 - val_loss: 0.7315 - val_acc: 0.7653\n",
      "Epoch 6/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.7653119633657699 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 104ms/step - loss: 0.9512 - acc: 0.6869 - val_loss: 1.2984 - val_acc: 0.6474\n",
      "Epoch 7/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.7784773898111047 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 0.8959 - acc: 0.7078 - val_loss: 0.6823 - val_acc: 0.7785\n",
      "Epoch 8/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.7784773898111047 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 104ms/step - loss: 0.8365 - acc: 0.7158 - val_loss: 0.8054 - val_acc: 0.7407\n",
      "Epoch 9/1000\n",
      "55/55 [==============================] - 3s 47ms/step\n",
      "max Accuracy:  0.7784773898111047 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 104ms/step - loss: 0.8581 - acc: 0.7151 - val_loss: 0.7391 - val_acc: 0.7596\n",
      "Epoch 10/1000\n",
      "55/55 [==============================] - 3s 46ms/step\n",
      "max Accuracy:  0.8168288494562106 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 0.7978 - acc: 0.7338 - val_loss: 0.5781 - val_acc: 0.8168\n",
      "Epoch 11/1000\n",
      "55/55 [==============================] - 3s 45ms/step\n",
      "max Accuracy:  0.8322839152833429 Learning Rate:  0.0003\n",
      "240/240 [==============================] - 25s 105ms/step - loss: 0.7691 - acc: 0.7482 - val_loss: 0.5196 - val_acc: 0.8323\n",
      "Epoch 12/1000\n",
      "44/55 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "history = model.fit(images_train, labels_train_encoded, batch_size=16, epochs=1000, validation_data=(images_test, labels_test_encoded), callbacks=[Save_Multiclass_Metrics_By_Epoch((images_test,labels_test), n_classes=10, save_after=1, save_csv_path=\"results_model_part1.csv\", model_name=\"model_part1.keras\", save_folder_name=\"Tomato\", min_rate=0.00004)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
