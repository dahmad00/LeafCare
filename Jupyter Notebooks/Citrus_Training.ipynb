{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1mah8we6k2Ts86CI5phjSEhpcm4XfbRux\n",
      "From (redirected): https://drive.google.com/uc?id=1mah8we6k2Ts86CI5phjSEhpcm4XfbRux&confirm=t&uuid=e3ac74a1-d025-4ebf-a55b-65f9f44fbb1d\n",
      "To: /workspace/CitrusV2.zip\n",
      "100%|██████████| 40.6M/40.6M [00:00<00:00, 66.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been successfully downloaded to: CitrusV2.zip\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# The extracted Google Drive file ID\n",
    "file_id = '1mah8we6k2Ts86CI5phjSEhpcm4XfbRux'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = 'CitrusV2.zip'  # Choose a name for your file\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(f\"File has been successfully downloaded to: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7dtX9thDxG_",
    "outputId": "d42931d8-61ca-4ca4-fa1f-9b52eb531f08"
   },
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"CitrusV2.zip\", 'r')\n",
    "files.extractall('Citrus')\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Citrus/CitrusV2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCitrus/CitrusV2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Citrus/CitrusV2'"
     ]
    }
   ],
   "source": [
    "os.listdir('Citrus/CitrusV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = '../workspace/Citrus/CitrusV1'\n",
    "\n",
    "# List all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    try:\n",
    "        # Check if it is a file\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"{file_path} has been deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QXl1wTYoDdBz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 09:49:17.235163: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 09:49:17.235230: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 09:49:17.235283: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_images(dir):\n",
    "\n",
    "    supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "    image_list = []\n",
    "    count = 0\n",
    "    # Walk through the directory and read images\n",
    "    for root, _, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            file_extension = os.path.splitext(file)[-1].lower()\n",
    "\n",
    "            # Check if the file is a .jpg or .jpeg image\n",
    "            if file_extension in supported_extensions:\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = imageio.imread(image_path)\n",
    "                    image1 = image\n",
    "\n",
    "                    image = np.asarray(image)\n",
    "                    del image1\n",
    "                    image_list.append(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading image {image_path}: {e}\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count % 100 == 0:\n",
    "              print(str(count) + \" images read\")\n",
    "\n",
    "\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hc04CAnpDnGK",
    "outputId": "2da9da6a-bcc3-4843-87df-9827c4ea2ba6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8GPaaxQYDqQM"
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def add_gaussian_noise(images, mean_range=(0, 15), std_range=(0, 0.15)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.AdditiveGaussianNoise(loc=mean_range, scale=(0, 0.2*255))\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def random_crop(images, crop_percent=(0.1, 0.4)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Crop(percent=crop_percent)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def random_rotate(images, rotation_range=(-360, 360)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Rotate(rotate=rotation_range)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def invert_images(images):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Add(value=(-20, 20))\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    # images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "def adjust_brightness(images, brightness_range=(-65, 65)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for adjusting brightness\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Multiply((1.0 + brightness_range[0] / 100.0, 1.0 + brightness_range[1] / 100.0))\n",
    "    ])\n",
    "\n",
    "\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "def scale_images(images, scale_factor = (0.3, 1.8)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for scaling images\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Affine(scale=scale_factor)\n",
    "    ])\n",
    "\n",
    "    augmented_images = [seq(image=image) for image in images]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def add_contrast(images, contrast_factor=(0.5, 1.5)):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for adding contrast\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.ContrastNormalization(alpha=contrast_factor)\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images_np]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def flip_images(images, flip_probability=0.5):\n",
    "    ia.seed(1)\n",
    "    # Define the augmentation pipeline for randomly flipping images\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Sometimes(flip_probability, iaa.Fliplr(1.0)),  # Horizontal flips\n",
    "        iaa.Sometimes(flip_probability, iaa.Flipud(1.0))   # Vertical flips\n",
    "    ])\n",
    "\n",
    "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
    "    images_np = np.array(images)\n",
    "\n",
    "    # Perform augmentation on each image individually\n",
    "    augmented_images = [seq(image=image) for image in images_np]\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "\n",
    "# discard images according to ratio\n",
    "def discard_images(images, discard_ratio=0.5):\n",
    "    random.seed(10)\n",
    "    # Calculate the number of images to discard based on the discard_ratio\n",
    "    num_images_to_discard = int(len(images) * discard_ratio)\n",
    "\n",
    "    # Create a copy of the input list to avoid modifying the original list\n",
    "    remaining_images = images[:]\n",
    "\n",
    "    # Randomly discard a portion of the images\n",
    "    random.shuffle(remaining_images)\n",
    "    remaining_images = remaining_images[num_images_to_discard:]\n",
    "\n",
    "    return remaining_images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resize_images(images_list, width=128, height=128):\n",
    "    ia.seed(1)\n",
    "    # Define the resize augmentation\n",
    "    resize_augmenter = iaa.Resize({\"height\": height, \"width\": width})\n",
    "\n",
    "    resized_images = []\n",
    "\n",
    "    for image in images_list:\n",
    "        # Ensure the image is in RGB format (imgaug uses RGB by default)\n",
    "        if image.shape[-1] == 1:  # Grayscale image with single channel\n",
    "            image = np.repeat(image, 3, axis=-1)\n",
    "\n",
    "        # Apply the resize augmentation\n",
    "        augmented_image = resize_augmenter.augment_image(image)\n",
    "\n",
    "        # Append the augmented image to the result list\n",
    "        resized_images.append(augmented_image)\n",
    "\n",
    "    del images_list[:]\n",
    "    return resized_images\n",
    "\n",
    "def keep_n_images(images, n_to_keep):\n",
    "    random.seed(10)\n",
    "    if n_to_keep >= len(images):\n",
    "        return images  # Keep all images if n_to_keep is greater than or equal to the image count\n",
    "\n",
    "    # Randomly shuffle the images list\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Keep the first n_to_keep images and discard the rest\n",
    "    kept_images = images[:n_to_keep]\n",
    "\n",
    "    # Create a copy of the kept images list\n",
    "    kept_images_copy = copy.deepcopy(kept_images)\n",
    "\n",
    "    # Clear the original images list to free memory\n",
    "    del images[:]\n",
    "\n",
    "    return kept_images_copy\n",
    "import cv2\n",
    "def normalize_images(image_list):\n",
    "\n",
    "  for i in range(len(image_list)):\n",
    "      image = image_list[i].astype(np.float32) / 255.0\n",
    "      image_list[i] = image\n",
    "\n",
    "def discard_images(lst, percent_to_discard):\n",
    "    # Calculate the number of images to discard\n",
    "    num_images_to_discard = int(len(lst) * percent_to_discard)\n",
    "    \n",
    "    # Generate a list of unique indices to discard\n",
    "    indices_to_discard = random.sample(range(len(lst)), num_images_to_discard)\n",
    "    \n",
    "    # For each index to discard, if there's a need for explicit clean-up (e.g., closing a file or freeing resources), do it here\n",
    "    for index in indices_to_discard:\n",
    "        image = lst[index]\n",
    "        # If the image is an object that requires closing or special disposal, do it here\n",
    "        # For example, if image objects are from PIL, you might need to call image.close() if they are open files\n",
    "        # Note: This is only necessary if your image objects require explicit clean-up\n",
    "    \n",
    "    # Create a new list excluding the indices to discard\n",
    "    modified_lst = [lst[i] for i in range(len(lst)) if i not in indices_to_discard]\n",
    "    \n",
    "    return modified_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOMlfHOEDeKR",
    "outputId": "f7460548-ae95-422e-cc07-51f934c72c6d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13428/1632636461.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(image_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images read\n",
      "100 images read\n",
      "100 images read\n",
      "Length of Black spot train: 118\n",
      "Length of Canker train: 113\n",
      "Length of Greening train: 142\n",
      "Length of Healthy train: 40\n",
      "Length of Melanose train: 8\n",
      "Length of Black spot test: 17\n",
      "Length of Canker test: 17\n",
      "Length of Greening test: 21\n",
      "Length of Healthy test: 6\n",
      "Length of Melanose test: 2\n",
      "Total length of all train parts: 421\n",
      "Total length of all test parts: 63\n",
      "Total length of all parts: 484\n"
     ]
    }
   ],
   "source": [
    "base_train_dir = '../workspace/Citrus/CitrusV1/Train/'\n",
    "base_test_dir = '../workspace/Citrus/CitrusV1/Val/'\n",
    "\n",
    "# Train data directories\n",
    "black_spot_train = read_images(os.path.join(base_train_dir, 'Black spot'))\n",
    "canker_train = read_images(os.path.join(base_train_dir, 'Canker'))\n",
    "greening_train = read_images(os.path.join(base_train_dir, 'Greening'))\n",
    "healthy_train = read_images(os.path.join(base_train_dir, 'Healthy'))  # Included explicitly for the 'Healthy' class\n",
    "melanose_train = read_images(os.path.join(base_train_dir, 'Melanose'))\n",
    "\n",
    "# Test data directories\n",
    "black_spot_test = read_images(os.path.join(base_test_dir, 'Black spot'))\n",
    "canker_test = read_images(os.path.join(base_test_dir, 'Canker'))\n",
    "greening_test = read_images(os.path.join(base_test_dir, 'Greening'))\n",
    "healthy_test = read_images(os.path.join(base_test_dir, 'Healthy'))  # Included explicitly for the 'Healthy' class\n",
    "melanose_test = read_images(os.path.join(base_test_dir, 'Melanose'))\n",
    "\n",
    "# For training classes\n",
    "print(\"Length of Black spot train:\", len(black_spot_train))\n",
    "print(\"Length of Canker train:\", len(canker_train))\n",
    "print(\"Length of Greening train:\", len(greening_train))\n",
    "print(\"Length of Healthy train:\", len(healthy_train))\n",
    "print(\"Length of Melanose train:\", len(melanose_train))\n",
    "\n",
    "# For test classes\n",
    "print(\"Length of Black spot test:\", len(black_spot_test))\n",
    "print(\"Length of Canker test:\", len(canker_test))\n",
    "print(\"Length of Greening test:\", len(greening_test))\n",
    "print(\"Length of Healthy test:\", len(healthy_test))\n",
    "print(\"Length of Melanose test:\", len(melanose_test))\n",
    "\n",
    "# Calculate the lengths of all train parts\n",
    "train_lengths = len(black_spot_train) + len(canker_train) + len(greening_train) + \\\n",
    "                len(healthy_train) + len(melanose_train)\n",
    "\n",
    "# Calculate the lengths of all test parts\n",
    "test_lengths = len(black_spot_test) + len(canker_test) + len(greening_test) + \\\n",
    "               len(healthy_test) + len(melanose_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Total length of all train parts:\", train_lengths)\n",
    "print(\"Total length of all test parts:\", test_lengths)\n",
    "\n",
    "# Calculate the total length of all parts\n",
    "total_length = train_lengths + test_lengths\n",
    "\n",
    "# Print the results\n",
    "print(\"Total length of all parts:\", total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black spot', 'Canker', 'Greening', 'Healthy', 'Melanose']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('Citrus/CitrusV1/Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTks90BUDq62",
    "outputId": "b73b85b4-ff77-49ad-f68b-17a80d7564be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13428/1632636461.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(image_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images read\n",
      "200 images read\n",
      "300 images read\n",
      "400 images read\n",
      "500 images read\n",
      "600 images read\n",
      "700 images read\n",
      "800 images read\n",
      "900 images read\n",
      "1000 images read\n",
      "1100 images read\n",
      "100 images read\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resize_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m canker_train_resized \u001b[38;5;241m=\u001b[39m resize_images(canker_train, dim, dim)\n\u001b[1;32m     77\u001b[0m greening_train_resized \u001b[38;5;241m=\u001b[39m resize_images(greening_train, dim, dim)\n\u001b[0;32m---> 78\u001b[0m healthy_train_resized \u001b[38;5;241m=\u001b[39m \u001b[43mresize_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhealthy_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m melanose_train_resized \u001b[38;5;241m=\u001b[39m resize_images(melanose_train, dim, dim)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Concatenate resized training images\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36mresize_images\u001b[0;34m(images, width, height)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_images\u001b[39m(images, width, height):\n\u001b[0;32m---> 71\u001b[0m     resized_images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresize_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized_images\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_images\u001b[39m(images, width, height):\n\u001b[0;32m---> 71\u001b[0m     resized_images \u001b[38;5;241m=\u001b[39m [\u001b[43mresize_image\u001b[49m(image, width, height) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized_images\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resize_image' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Assuming base_train_dir and base_test_dir variables are already defined\n",
    "base_train_dir = 'Tomato/Train'\n",
    "base_test_dir = 'Tomato/Val'\n",
    "\n",
    "# Function to process and augment images for a given class\n",
    "def process_images(class_name, base_dir):\n",
    "    images = read_images(os.path.join(base_dir, class_name))\n",
    "    images_all = (\n",
    "        add_gaussian_noise(images) +\n",
    "        random_crop(images) +\n",
    "        invert_images(images) +\n",
    "        adjust_brightness(images) +\n",
    "        scale_images(images) +\n",
    "        random_rotate(images)\n",
    "    )\n",
    "    return images + images_all\n",
    "\n",
    "# Process training images for new classes\n",
    "black_spot_train = process_images('Black_spot', base_train_dir)\n",
    "canker_train = process_images('Canker', base_train_dir)\n",
    "greening_train = process_images('Greening', base_train_dir)\n",
    "healthy_train = process_images('Healthy', base_train_dir)  # 'Healthy' class is common\n",
    "melanose_train = process_images('Melanose', base_train_dir)\n",
    "\n",
    "# Process testing images for new classes\n",
    "black_spot_test = process_images('Black_spot', base_test_dir)\n",
    "canker_test = process_images('Canker', base_test_dir)\n",
    "greening_test = process_images('Greening', base_test_dir)\n",
    "healthy_test = process_images('Healthy', base_test_dir)  # 'Healthy' class is common\n",
    "melanose_test = process_images('Melanose', base_test_dir)\n",
    "\n",
    "# Generate training labels\n",
    "labels_black_spot_train = np.zeros(len(black_spot_train))\n",
    "labels_canker_train = np.ones(len(canker_train))\n",
    "labels_greening_train = np.full(len(greening_train), 2)\n",
    "labels_healthy_train = np.full(len(healthy_train), 3)\n",
    "labels_melanose_train = np.full(len(melanose_train), 4)\n",
    "\n",
    "# Combine training labels\n",
    "labels_train = np.concatenate([\n",
    "    labels_black_spot_train,\n",
    "    labels_canker_train,\n",
    "    labels_greening_train,\n",
    "    labels_healthy_train,\n",
    "    labels_melanose_train\n",
    "])\n",
    "\n",
    "# Generate testing labels\n",
    "labels_black_spot_test = np.zeros(len(black_spot_test))\n",
    "labels_canker_test = np.ones(len(canker_test))\n",
    "labels_greening_test = np.full(len(greening_test), 2)\n",
    "labels_healthy_test = np.full(len(healthy_test), 3)\n",
    "labels_melanose_test = np.full(len(melanose_test), 4)\n",
    "\n",
    "# Combine testing labels\n",
    "labels_test = np.concatenate([\n",
    "    labels_black_spot_test,\n",
    "    labels_canker_test,\n",
    "    labels_greening_test,\n",
    "    labels_healthy_test,\n",
    "    labels_melanose_test\n",
    "])\n",
    "\n",
    "dim = 224  # Desired size for resizing images\n",
    "\n",
    "# Function to resize images\n",
    "def resize_images(images, width, height):\n",
    "    resized_images = [resize_image(image, width, height) for image in images]\n",
    "    return resized_images\n",
    "\n",
    "# Resize training images\n",
    "black_spot_train_resized = resize_images(black_spot_train, dim, dim)\n",
    "canker_train_resized = resize_images(canker_train, dim, dim)\n",
    "greening_train_resized = resize_images(greening_train, dim, dim)\n",
    "healthy_train_resized = resize_images(healthy_train, dim, dim)\n",
    "melanose_train_resized = resize_images(melanose_train, dim, dim)\n",
    "\n",
    "# Concatenate resized training images\n",
    "images_train = np.concatenate([\n",
    "    black_spot_train_resized,\n",
    "    canker_train_resized,\n",
    "    greening_train_resized,\n",
    "    healthy_train_resized,\n",
    "    melanose_train_resized\n",
    "])\n",
    "\n",
    "# Resize testing images\n",
    "black_spot_test_resized = resize_images(black_spot_test, dim, dim)\n",
    "canker_test_resized = resize_images(canker_test, dim, dim)\n",
    "greening_test_resized = resize_images(greening_test, dim, dim)\n",
    "healthy_test_resized = resize_images(healthy_test, dim, dim)\n",
    "melanose_test_resized = resize_images(melanose_test, dim, dim)\n",
    "\n",
    "# Concatenate resized testing images\n",
    "images_test = np.concatenate([\n",
    "    black_spot_test_resized,\n",
    "    canker_test_resized,\n",
    "    greening_test_resized,\n",
    "    healthy_test_resized,\n",
    "    melanose_test_resized\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Correcting the discard percentage to match the description\n",
    "# # discard_percentage = 0.9\n",
    "\n",
    "# # Overwrite each class variable with the reduced dataset after discarding 52% of images\n",
    "# # bacterial_spot_all = discard_images(bacterial_spot_all, 1)\n",
    "# early_blight_all = discard_images(early_blight_all, 0.81)\n",
    "# healthy_all = discard_images(healthy_all, 0.93) \n",
    "# # late_blight_all = discard_images(late_blight_all, 1)\n",
    "# leaf_mold_all = discard_images(leaf_mold_all, 0.8)\n",
    "# mosaic_virus_all = discard_images(mosaic_virus_all, 0.16)\n",
    "# septoria_leaf_spot_all = discard_images(septoria_leaf_spot_all, 0.96)\n",
    "# spider_mites_all = discard_images(spider_mites_all, 0.96)\n",
    "# target_spot_all = discard_images(target_spot_all, 0.91)\n",
    "# # yellow_leaf_curl_virus_all = discard_images(yellow_leaf_curl_virus_all, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaxJ-J5qk8QI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define the percentage of images to discard\n",
    "# discard_percentage = 0.7  # This means 50% of the data will be discarded\n",
    "\n",
    "# # Apply the discard_images function to each dataset with the defined discard percentage\n",
    "# bacterial_spot_train = discard_images(bacterial_spot_train, discard_percentage)\n",
    "# early_blight_train = discard_images(early_blight_train, discard_percentage)\n",
    "# healthy_train = discard_images(healthy_train, discard_percentage)\n",
    "# late_blight_train = discard_images(late_blight_train, discard_percentage)\n",
    "# leaf_mold_train = discard_images(leaf_mold_train, discard_percentage)\n",
    "# mosaic_virus_train = discard_images(mosaic_virus_train, discard_percentage)\n",
    "# septoria_leaf_spot_train = discard_images(septoria_leaf_spot_train, discard_percentage)\n",
    "# spider_mites_train = discard_images(spider_mites_train, discard_percentage)\n",
    "# target_spot_train = discard_images(target_spot_train, discard_percentage)\n",
    "# yellow_leaf_curl_virus_train = discard_images(yellow_leaf_curl_virus_train, discard_percentage)\n",
    "\n",
    "# # After executing the above lines, each dataset variable will now reference a subset\n",
    "# that retains only 50% of the original images, since we're\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBt4R0viD3Pz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fm18BEYAxqG9"
   },
   "outputs": [],
   "source": [
    "\n",
    "normalize_images(images_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzUKhudfCMuR",
    "outputId": "3c35d115-5756-4376-f26e-366e44aee695"
   },
   "outputs": [],
   "source": [
    "images_train[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVOutQGDxwfT"
   },
   "outputs": [],
   "source": [
    "normalize_images(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKXb38zpD6tC"
   },
   "outputs": [],
   "source": [
    "#TEST Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X125FDBD9Pd",
    "outputId": "b7d3f1ef-a11b-4c69-b687-dc8c6dc67785"
   },
   "outputs": [],
   "source": [
    "print(len(images_train))\n",
    "print(len(labels_train))\n",
    "\n",
    "counts = [0,0,0,0,0]\n",
    "\n",
    "for label in labels_train:\n",
    "    counts[int(label)] += 1\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS_dvGp1D_zT"
   },
   "outputs": [],
   "source": [
    "\n",
    "ls1 = images_test\n",
    "ls2 = images_train\n",
    "\n",
    "images_test = np.array(images_test)\n",
    "images_train = np.array(images_train)\n",
    "\n",
    "del ls1\n",
    "del ls2\n",
    "\n",
    "\n",
    "shuffle_indices_train = np.random.permutation(len(images_train))\n",
    "shuffle_indices_test = np.random.permutation(len(images_test))\n",
    "\n",
    "\n",
    "images_train = images_train[shuffle_indices_train]\n",
    "labels_train = labels_train[shuffle_indices_train]\n",
    "\n",
    "images_test = images_test[shuffle_indices_test]\n",
    "labels_test = labels_test[shuffle_indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NsYhuwREBCD"
   },
   "outputs": [],
   "source": [
    "# %load custom_callback.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Find accuracy of model\n",
    "def find_accuracy(test,pred):\n",
    "    correct = 0\n",
    "    total = len(test)\n",
    "\n",
    "    for i in range(len(test)):\n",
    "        if test[i] == pred[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "# Map ANN outputs to classes\n",
    "def get_labels(y_pred_ann):\n",
    "    labels = []\n",
    "\n",
    "    for pred in y_pred_ann:\n",
    "        max_index = 0\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] > pred[max_index]:\n",
    "                max_index = i\n",
    "\n",
    "        labels.append(max_index)\n",
    "\n",
    "    return labels\n",
    "\n",
    "# This callback prints accuracy by epoch information after each epoch\n",
    "class Save_Accuracy_By_Epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.X_Test = test_data[0]\n",
    "        self.Y_Test = test_data[1]\n",
    "        self.accuracies = []\n",
    "        self.epochs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        y_pred = self.model.predict(self.X_Test)\n",
    "\n",
    "        if epoch == 4:\n",
    "            pass\n",
    "\n",
    "        y_pred = get_labels(y_pred)\n",
    "        accuracy = find_accuracy(self.Y_Test, y_pred)\n",
    "        self.epochs.append(epoch+1)\n",
    "        self.accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        print(self.epochs)\n",
    "        print(self.accuracies)\n",
    "\n",
    "\n",
    "# This callback prints metrics for every class after each epoch\n",
    "class Save_Multiclass_Metrics_By_Epoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data, n_classes, save_after = 10,save_thresh =0.94, save_csv_path = 'results.csv', model_name = 'model.keras', decay_rate = 0.9, min_rate = 0.000032, save_folder_name = \".\"):\n",
    "        self.X_Test = test_data[0]\n",
    "        self.Y_Test = test_data[1]\n",
    "\n",
    "        self.min_rate = min_rate;\n",
    "        \n",
    "        self.epochs = []\n",
    "        self.n_classes = n_classes\n",
    "        self.save_after = save_after\n",
    "        self.save_csv_path = save_csv_path\n",
    "        self.save_folder_name = save_folder_name\n",
    "        self.model_name = model_name\n",
    "        self.max_accuracy = 0\n",
    "        self.save_thresh = save_thresh\n",
    "\n",
    "        self.mat_sensitivity = []\n",
    "        self.mat_specificity = []\n",
    "        self.mat_precision = []\n",
    "        self.mat_recall = []\n",
    "        self.mat_accuracy = []\n",
    "        self.mat_f1 = []\n",
    "        self.accuracies = []\n",
    "\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.mat_sensitivity.append([])\n",
    "            self.mat_specificity.append([])\n",
    "            self.mat_precision.append([])\n",
    "            self.mat_recall.append([])\n",
    "            self.mat_accuracy.append([])\n",
    "            self.mat_f1.append([])\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        y_pred = self.model.predict(self.X_Test)\n",
    "        y_pred = get_labels(y_pred)\n",
    "\n",
    "        total = len(self.Y_Test)\n",
    "\n",
    "        correct = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == self.Y_Test[i]:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct/total\n",
    "        self.accuracies.append(correct / total)\n",
    "\n",
    "        best = False\n",
    "\n",
    "        if accuracy >= self.max_accuracy:\n",
    "            self.max_accuracy = accuracy\n",
    "            best = True\n",
    "\n",
    "\n",
    "        if accuracy > self.save_thresh:\n",
    "          best = True\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "            FN = 0\n",
    "\n",
    "            for j in range(len(y_pred)):\n",
    "                if self.Y_Test[j] == i and y_pred[j] == i:\n",
    "                    TP += 1\n",
    "                elif self.Y_Test[j] != i and y_pred[j] == i:\n",
    "                    FP += 1\n",
    "                elif self.Y_Test[j] == i and y_pred[j] != i:\n",
    "                    FN += 1\n",
    "                elif self.Y_Test[j] != i and y_pred[j] != i:\n",
    "                    TN += 1\n",
    "\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else -1\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else -1\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
    "            accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else -1\n",
    "\n",
    "            self.mat_sensitivity[i].append(sensitivity)\n",
    "            self.mat_specificity[i].append(specificity)\n",
    "            self.mat_precision[i].append(precision)\n",
    "            self.mat_recall[i].append(recall)\n",
    "            self.mat_accuracy[i].append(accuracy)\n",
    "            self.mat_f1[i].append(f1)\n",
    "\n",
    "        self.epochs.append(int(epoch+1))\n",
    "\n",
    "        print(\"max Accuracy: \", self.max_accuracy, \"Learning Rate: \",self.model.optimizer.learning_rate.numpy())\n",
    "           \n",
    "\n",
    "        if self.model.optimizer.learning_rate <= self.min_rate :\n",
    "           self.model.stop_training = True\n",
    "\n",
    "        if (epoch + 1) % self.save_after == 0:\n",
    "            save_to_csv_file(self.save_csv_path, self.mat_sensitivity, self.mat_specificity, self.mat_precision, self.mat_recall, self.mat_accuracy, self.mat_f1, self.accuracies, self.epochs)\n",
    "            self.model.save(self.model_name)\n",
    "            if best:\n",
    "                self.model.save(self.save_folder_name + \"/epoch\" + str((epoch+1)) + \"_\" + self.model_name)\n",
    "            pass\n",
    "        \n",
    "\n",
    " \n",
    "\n",
    "def save_to_csv_file(path, mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1, accuracies, epochs):\n",
    "    mat_sensitivity = np.transpose(mat_sensitivity)\n",
    "    mat_specificity = np.transpose(mat_specificity)\n",
    "    mat_precision = np.transpose(mat_precision)\n",
    "    mat_recall = np.transpose(mat_recall)\n",
    "    mat_accuracy = np.transpose(mat_accuracy)\n",
    "    mat_f1 = np.transpose(mat_f1)\n",
    "    accuracies = np.reshape(accuracies, (-1,1))\n",
    "    epochs = np.reshape(epochs, (-1,1))\n",
    "\n",
    "    mat_join = np.concatenate((epochs,accuracies,mat_sensitivity, mat_specificity, mat_precision, mat_recall, mat_accuracy, mat_f1), axis = 1)\n",
    "\n",
    "    n = mat_sensitivity.shape[1]\n",
    "\n",
    "\n",
    "    col_array_sensitivity = list(range(n))\n",
    "    col_array_specificity = list(range(n))\n",
    "    col_array_precision = list(range(n))\n",
    "    col_array_recall = list(range(n))\n",
    "    col_array_accuracy = list(range(n))\n",
    "    col_array_f1 = list(range(n))\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        col_array_sensitivity[i] = 'sensitivity Class ' + str(col_array_sensitivity[i])\n",
    "        col_array_specificity[i] = 'specificity Class ' + str(col_array_specificity[i])\n",
    "        col_array_precision[i] = 'precision Class ' + str(col_array_precision[i])\n",
    "        col_array_recall[i] = 'recall Class ' + str(col_array_recall[i])\n",
    "        col_array_accuracy[i] = 'accuracy Class ' + str(col_array_accuracy[i])\n",
    "        col_array_f1[i] = 'f1 Class ' + str(col_array_f1[i])\n",
    "\n",
    "    cols = ['Epoch']+['overall_accuracy']+ col_array_sensitivity + col_array_specificity + col_array_precision + col_array_recall + col_array_accuracy + col_array_f1\n",
    "\n",
    "    mat_join = np.flip(mat_join, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns = cols,\n",
    "        data  = mat_join\n",
    "    )\n",
    "\n",
    "    df.to_csv(path, index= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ewhCiGnECZl"
   },
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "labels_train_encoded = []\n",
    "for label in labels_train:\n",
    "    encoding = [0,0,0,0,0]\n",
    "    encoding[int(label)] = 1\n",
    "    labels_train_encoded.append(encoding)\n",
    "\n",
    "labels_train_encoded = np.array(labels_train_encoded)\n",
    "\n",
    "labels_test_encoded = []\n",
    "for label in labels_test:\n",
    "    encoding = [0,0,0,0,0]\n",
    "    encoding[int(label)] = 1\n",
    "    labels_test_encoded.append(encoding)\n",
    "\n",
    "labels_test_encoded = np.array(labels_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwL3tWAZ06SW"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D,  \\\n",
    "    Dropout, Dense, Input, concatenate,      \\\n",
    "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
    "    Flatten, BatchNormalization, Activation, MaxPooling2D, GlobalMaxPooling2D,\\\n",
    "    Reshape,  multiply, add, Permute\n",
    "\n",
    "import math\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "CE-r9jApzZPw",
    "outputId": "e68e56bd-6e0b-48ff-c8d8-859baed905af"
   },
   "outputs": [],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEgZe_RUpwwU"
   },
   "outputs": [],
   "source": [
    "def _conv2d_bn(input,\n",
    "               filters,\n",
    "               num_row,\n",
    "               num_col,\n",
    "               padding='same',\n",
    "               strides=(1, 1),\n",
    "               name=None):\n",
    "\n",
    "    if name is not None:\n",
    "        bn_name = '{name}_bn'.format(name=name)\n",
    "        conv_name = '{name}_conv'.format(name=name)\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    X = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name) (input)\n",
    "    X = BatchNormalization(axis=bn_axis, scale=False, name=bn_name) (X)\n",
    "    X = Activation('relu', name=name)(X)\n",
    "\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOJJDx6JS7vm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _tensor_shape(tensor):\n",
    "    return getattr(tensor, 'shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lac-X6wtot2p"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "raw_model = tf.keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "model.add(raw_model)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D65LqJMLpa75",
    "outputId": "822f0c2e-7870-4865-f727-56d5c38321f4"
   },
   "outputs": [],
   "source": [
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.0003,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "tf.keras.utils.set_random_seed(2)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss=loss, metrics = ['acc'])\n",
    "# model.compile(optimizer='adam', loss=loss, metrics = ['acc'], loss_weights=class_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYya-q-9BNKB",
    "outputId": "f94d2482-ffac-4d00-ece2-a39d7a64bf2c"
   },
   "outputs": [],
   "source": [
    "history = model.fit(images_train, labels_train_encoded, batch_size=16, epochs=1000, validation_data=(images_test, labels_test_encoded), callbacks=[Save_Multiclass_Metrics_By_Epoch((images_test,labels_test), n_classes=10, save_after=1, save_csv_path=\"results_model_part1.csv\", model_name=\"model_part1.keras\", save_folder_name=\"Model\", min_rate=0.000004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('../workspace/Model/epoch99_model_part1.keras')\n",
    "# model.save('tomato.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
