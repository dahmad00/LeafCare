{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3qfyu_SVNVc"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D,  \\\n",
        "    Dropout, Dense, Input, concatenate,      \\\n",
        "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
        "    Flatten, BatchNormalization, Activation, MaxPooling2D, GlobalMaxPooling2D,\\\n",
        "    Reshape,  multiply, add, Permute\n",
        "\n",
        "import math\n",
        "# from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.14"
      ],
      "metadata": {
        "id": "LFhpAILAE5NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew3Y23OJWVK-",
        "outputId": "5061231f-a426-40ec-c23e-a8eef5467eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rog2yJEsWt5J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_images(dir):\n",
        "\n",
        "    supported_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
        "    image_list = []\n",
        "    count = 0\n",
        "    # Walk through the directory and read images\n",
        "    for root, _, files in os.walk(dir):\n",
        "        for file in files:\n",
        "            file_extension = os.path.splitext(file)[-1].lower()\n",
        "\n",
        "            # Check if the file is a .jpg or .jpeg image\n",
        "            if file_extension in supported_extensions:\n",
        "                image_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    image = imageio.imread(image_path)\n",
        "                    image1 = image\n",
        "\n",
        "                    image = np.asarray(image)\n",
        "                    del image1\n",
        "                    image_list.append(image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading image {image_path}: {e}\")\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if count % 100 == 0:\n",
        "              print(str(count) + \" images read\")\n",
        "\n",
        "\n",
        "    return image_list\n",
        "\n",
        "\n",
        "def random_rotate(images, rotation_range=(-360, 360)):\n",
        "    ia.seed(1)\n",
        "    # Define the augmentation pipeline\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Rotate(rotate=rotation_range)\n",
        "    ])\n",
        "\n",
        "    # Convert images to numpy array (imgaug requires numpy arrays)\n",
        "    # images_np = np.array(images)\n",
        "\n",
        "    # Perform augmentation on each image individually\n",
        "    augmented_images = [seq(image=image) for image in images]\n",
        "\n",
        "    return augmented_images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fjAwWcNWbUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2dec63-b6d9-493b-a160-775e90f3c4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['serious_cedar_rust', 'grey_spot', 'serious_scab', 'healthy', 'general_cedar_rust', 'general_scab']\n"
          ]
        }
      ],
      "source": [
        "dir = os.listdir('drive/MyDrive/testing datasets/apples/test')\n",
        "print(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSrdoeWsWI2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c62e80d-a3f9-4c31-b6fa-54ab6c7579a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-2ca625451841>:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(image_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 images read\n",
            "200 images read\n"
          ]
        }
      ],
      "source": [
        "# Train data directories\n",
        "base_train_dir = 'drive/MyDrive/testing datasets/apples/test'\n",
        "\n",
        "general_cedar_rust = read_images(os.path.join(base_train_dir, 'general_cedar_rust'))\n",
        "general_scab = read_images(os.path.join(base_train_dir, 'general_scab'))\n",
        "grey_spot = read_images(os.path.join(base_train_dir, 'grey_spot'))\n",
        "healthy_apple = read_images(os.path.join(base_train_dir, 'healthy'))\n",
        "serious_cedar_rust = read_images(os.path.join(base_train_dir, 'serious_cedar_rust'))\n",
        "serious_scab = read_images(os.path.join(base_train_dir, 'serious_scab'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-LrZYxWHrcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSlClQJ-WUj4"
      },
      "outputs": [],
      "source": [
        "labels_healthy_apple = np.zeros(len(healthy_apple))\n",
        "labels_general_scab = np.ones(len(general_scab))\n",
        "labels_serious_scab = np.full(len(serious_scab), 2)\n",
        "labels_grey_spot = np.full(len(grey_spot), 3)\n",
        "labels_general_cedar_rust = np.full(len(general_cedar_rust),4)\n",
        "labels_serious_cedar_rust = np.full(len(serious_cedar_rust), 5)\n",
        "\n",
        "\n",
        "# Combine train labels\n",
        "labels = np.concatenate([\n",
        "    labels_healthy_apple,\n",
        "    labels_general_scab,\n",
        "    labels_serious_scab,\n",
        "    labels_grey_spot,\n",
        "    labels_general_cedar_rust,\n",
        "    labels_serious_cedar_rust,\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwOdSjMrHsYw"
      },
      "outputs": [],
      "source": [
        "# Relative Frequencies\n",
        "# Create an array to store the frequencies of the lists\n",
        "class_freq = []\n",
        "\n",
        "# Append the lengths of the lists to the array\n",
        "class_freq.append(len(healthy_apple))\n",
        "class_freq.append(len(general_scab))\n",
        "class_freq.append(len(serious_scab))\n",
        "class_freq.append(len(grey_spot))\n",
        "class_freq.append(len(general_cedar_rust))\n",
        "class_freq.append(len(serious_cedar_rust))\n",
        "\n",
        "# Calculate the sum of all frequencies\n",
        "total_freq = sum(class_freq)\n",
        "\n",
        "# Calculate the relative frequencies\n",
        "relative_freq = [freq / total_freq for freq in class_freq]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSgWfFRIXmkv"
      },
      "outputs": [],
      "source": [
        "images =healthy_apple + general_scab+serious_scab  + grey_spot+ general_cedar_rust  +  serious_cedar_rust\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('drive/MyDrive/A2_Models/')"
      ],
      "metadata": {
        "id": "Mq5Epu1IDhff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5cbaa9-6e21-4f0c-8fc7-5270e758e0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_epoch23_densenet121_part2.keras',\n",
              " 'best_epoch16_densenet121_part1.keras',\n",
              " 'best_epoch9_densenet121_part1.keras',\n",
              " 'gamma1_best_epoch21_densenet121_part1.keras',\n",
              " 'gamma01_best_epoch17_densenet121_part1.keras',\n",
              " 'batch64_best_epoch17_densenet121_part1.keras',\n",
              " 'best_epoch42_densenet121expanded_part1.keras',\n",
              " 'gamma1best_epoch26_densenet121expanded_part3.keras',\n",
              " 'densenet121expanded_part3.keras',\n",
              " 'gamma01best_epoch8_densenet121expanded_part2.keras',\n",
              " 'gamma2best_epoch18_densenet121frozen_part1.keras.keras',\n",
              " 'gamma2best_epoch18_densenet121frozen_part1.keras',\n",
              " 'gamma1best_epoch17_densenet121frozen_part3.keras',\n",
              " 'gamma01best_epoch36_densenet121frozen_part2.keras.keras',\n",
              " 'crossbest_epoch46_densenet121frozen_part1.keras',\n",
              " 'gamma2best_epoch22_sedensenet121_part2.keras',\n",
              " 'gamma1best_epoch20_sedensenet121_part2.keras',\n",
              " 'gamma01best_epoch20_sedensenet121_part2.keras',\n",
              " 'crossbest_epoch20_sedensenet121_part2.keras',\n",
              " 'gamma2best_epoch3_densenet121frozen_part2.keras',\n",
              " 'crossbest_epoch41_customModel_part2.keras',\n",
              " 'gamma1best_epoch17_CustomModel_part3.keras',\n",
              " 'crossweightsbest_epoch14_CustomModel_part3 (2).keras',\n",
              " 'crossweightsbest_epoch14_CustomModel_part3 (1).keras',\n",
              " 'crossweightsbest_epoch14_CustomModel_part3.keras',\n",
              " 'gamma02best_epoch29_InceptionV3_part2.keras',\n",
              " 'gamma01best_epoch6_InceptionV3_part2.keras',\n",
              " 'gamma01best_epoch22_InceptionV3_part3.keras',\n",
              " 'crossbest_epoch14_InceptionV3_part2.keras',\n",
              " 'crossbbest_epoch23_results_VGG16_part1 (1).keras',\n",
              " 'crossbbest_epoch23_results_VGG16_part1.keras',\n",
              " 'gamma2best_epoch23_vgg16_part1.keras',\n",
              " 'gamma1best_epoch8_vgg16_part1.keras',\n",
              " 'gamma01best_epoch5_vgg16_part1.keras',\n",
              " 'gamma01best_epoch_Mobilenet_part1.keras',\n",
              " 'test.keras',\n",
              " 'gamma2best_epoch8_mobilenet_part2.keras',\n",
              " 'gamma1best_epoch20_mobilenet_part2.keras',\n",
              " 'crossbest_epoch2_mobilenet_part2.keras',\n",
              " 'gamma2best_epoch5_xception_part2.keras',\n",
              " 'gamma1best_epoch_xception_part2.keras',\n",
              " 'gamma01best_epoch_customModel_part2_v2.keras',\n",
              " 'gamma01best_epoch3_CustomModel_part2.keras',\n",
              " 'gamma01best_epoch7_xception_part1.keras',\n",
              " 'crossbest_epoch24_xception_part1.keras']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.14"
      ],
      "metadata": {
        "id": "Iv2RrCBWjRY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e9d23b-141b-48a0-c23c-b3ae26172ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14 in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxFNxx5VXrCj"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('epoch366_apple_without_aug_model_part2.keras')\n",
        "\n",
        "# model = keras.models.load_model('drive/MyDrive/A2_Models/crossbest_epoch24_xception_part1.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWNtSlRbX7f0"
      },
      "outputs": [],
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Find accuracy of model\n",
        "def find_accuracy(test,pred):\n",
        "    correct = 0\n",
        "    total = len(test)\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        if test[i] == pred[i]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "\n",
        "# Map ANN outputs to classes\n",
        "def get_labels(y_pred_ann):\n",
        "    labels = []\n",
        "\n",
        "    for pred in y_pred_ann:\n",
        "        max_index = 0\n",
        "\n",
        "        for i in range(len(pred)):\n",
        "            if pred[i] > pred[max_index]:\n",
        "                max_index = i\n",
        "\n",
        "        labels.append(max_index)\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "def resize_images(images_list, width=128, height=128):\n",
        "    ia.seed(1)\n",
        "    # Define the resize augmentation\n",
        "    resize_augmenter = iaa.Resize({\"height\": height, \"width\": width})\n",
        "\n",
        "    resized_images = []\n",
        "\n",
        "    for image in images_list:\n",
        "        # Ensure the image is in RGB format (imgaug uses RGB by default)\n",
        "        if image.shape[-1] == 1:  # Grayscale image with single channel\n",
        "            image = np.repeat(image, 3, axis=-1)\n",
        "\n",
        "        # Apply the resize augmentation\n",
        "        augmented_image = resize_augmenter.augment_image(image)\n",
        "\n",
        "        # Append the augmented image to the result list\n",
        "        resized_images.append(augmented_image)\n",
        "\n",
        "    return resized_images\n",
        "\n",
        "def normalize_images(image_list):\n",
        "  for i in range(len(image_list)):\n",
        "    image = image_list[i].astype(np.float32) / 255.0\n",
        "    image_list[i] = image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOPkzwDPYCEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbd57e5-30d3-47aa-cd4b-a3b6206fe505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 32s 2s/step\n"
          ]
        }
      ],
      "source": [
        "images = resize_images(images, 128, 128)\n",
        "normalize_images(images)\n",
        "images = np.array(images)\n",
        "\n",
        "pred = model.predict(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdDCOAA8pjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00517f50-9c33-4adc-ca90-7c7e0e18b364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "455\n",
            "493\n",
            "0.922920892494929\n"
          ]
        }
      ],
      "source": [
        "n_classes = 6\n",
        "\n",
        "y_pred = get_labels(pred)\n",
        "total = len(y_pred)\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] == labels[i]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "print(correct)\n",
        "print(total)\n",
        "\n",
        "# Initialize variables to store weighted sums\n",
        "weighted_sum_sensitivity = 0\n",
        "weighted_sum_specificity = 0\n",
        "weighted_sum_precision = 0\n",
        "weighted_sum_accuracy = 0\n",
        "weighted_sum_f1 = 0\n",
        "\n",
        "class_metrics = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "\n",
        "    for j in range(len(y_pred)):\n",
        "        if labels[j] == i and y_pred[j] == i:\n",
        "            TP += 1\n",
        "        elif labels[j] != i and y_pred[j] == i:\n",
        "            FP += 1\n",
        "        elif labels[j] == i and y_pred[j] != i:\n",
        "            FN += 1\n",
        "        elif labels[j] != i and y_pred[j] != i:\n",
        "            TN += 1\n",
        "\n",
        "\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else -1\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else -1\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else -1\n",
        "    class_accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
        "    recall = sensitivity\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else -1\n",
        "\n",
        "    class_metrics.append({\n",
        "        'class': int(i),\n",
        "        'TP': TP,\n",
        "        'FP': FP,\n",
        "        'TN': TN,\n",
        "        'FN': FN,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Specificity': specificity,\n",
        "        'Precision': precision,\n",
        "        'Accuracy': class_accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'Relative Frequency': relative_freq[i]\n",
        "    })\n",
        "\n",
        "    # Calculate weighted sums\n",
        "    weighted_sum_sensitivity += sensitivity * relative_freq[i]\n",
        "    weighted_sum_specificity += specificity * relative_freq[i]\n",
        "    weighted_sum_precision += precision * relative_freq[i]\n",
        "    weighted_sum_accuracy += class_accuracy * relative_freq[i]\n",
        "    weighted_sum_f1 += f1 * relative_freq[i]\n",
        "\n",
        "\n",
        "\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_K512_yJ65Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6opZiaIyrSf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "labels_encoded = []\n",
        "for label in labels:\n",
        "    encoding = [0,0,0,0,0,0]\n",
        "    encoding[int(label)] = 1\n",
        "    labels_encoded.append(encoding)\n",
        "\n",
        "labels_encoded = np.array(labels_encoded)\n"
      ],
      "metadata": {
        "id": "-iJ0aUQLwMrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klocgJ91Iyir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0432ed-2f33-4ddb-99a2-ab85cfbc3aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 24s 1s/step - loss: 0.0919 - acc: 0.9229\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(images, labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa0RCULyyQcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1a0b8e-8df1-47be-de85-dcfdcf797885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Class  Specificity  Sensitivity  Precision  Accuracy  F1 Score\n",
            "0             0        0.977        0.967      0.981     0.972     0.974\n",
            "1             1        0.962        0.792      0.691     0.945     0.738\n",
            "2             2        0.987        0.647      0.786     0.963     0.710\n",
            "3             3        0.992        0.990      0.970     0.992     0.980\n",
            "4             4        0.989        0.939      0.861     0.986     0.899\n",
            "5             5        0.996        0.556      0.714     0.988     0.625\n",
            "6  Weighted Avg        0.981        0.923      0.924     0.974     0.922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-62d661c5ca5e>:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  metrics_dataframe = metrics_dataframe.append(weighted_avg_metrics, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_metrics_dataframe(class_metrics):\n",
        "    # Create a dictionary to store the metrics for each class\n",
        "    metrics_dict = {\n",
        "        'Class': [metrics['class'] for metrics in class_metrics],\n",
        "        'Specificity': [metrics['Specificity'] for metrics in class_metrics],\n",
        "        'Sensitivity': [metrics['Sensitivity'] for metrics in class_metrics],\n",
        "        'Precision': [metrics['Precision'] for metrics in class_metrics],\n",
        "        'Accuracy': [metrics['Accuracy'] for metrics in class_metrics],\n",
        "        'F1 Score': [metrics['F1 Score'] for metrics in class_metrics]\n",
        "    }\n",
        "\n",
        "    # Create a Pandas DataFrame from the metrics dictionary\n",
        "    metrics_df = pd.DataFrame(metrics_dict)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "# Usage:\n",
        "# Assuming you have class_metrics list defined as in your previous code\n",
        "metrics_dataframe = create_metrics_dataframe(class_metrics)\n",
        "\n",
        "# Calculate weighted average metrics\n",
        "weighted_avg_metrics = {\n",
        "    'Class': 'Weighted Avg',\n",
        "    'Specificity': weighted_sum_specificity,  # Replace with your actual values\n",
        "    'Sensitivity': weighted_sum_sensitivity,  # Replace with your actual values\n",
        "    'Precision': weighted_sum_precision,      # Replace with your actual values\n",
        "    'Accuracy': weighted_sum_accuracy,        # Replace with your actual values\n",
        "    'F1 Score': weighted_sum_f1               # Replace with your actual values\n",
        "}\n",
        "\n",
        "\n",
        "# Append the weighted average metrics to the dataframe\n",
        "metrics_dataframe = metrics_dataframe.append(weighted_avg_metrics, ignore_index=True)\n",
        "\n",
        "metrics_dataframe = metrics_dataframe.round(3)\n",
        "\n",
        "print(metrics_dataframe)\n",
        "\n",
        "metrics_dataframe.to_csv('classwise_metrics.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gqkCMHY0UY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2d98a4-a5f7-4f77-efe1-85c7a39ca15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "1 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "1 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "3 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "2 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "1 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "3 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "5 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "1 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "1 0.0\n",
            "0 0.0\n",
            "0 0.0\n",
            "3 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "2 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "2 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "2 1.0\n",
            "1 1.0\n",
            "2 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "0 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "0 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "2 1.0\n",
            "0 1.0\n",
            "0 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "1 1.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "1 2.0\n",
            "0 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "4 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "1 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "2 2.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "1 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "3 3.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "1 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "5 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "4 4.0\n",
            "5 5.0\n",
            "5 5.0\n",
            "4 5.0\n",
            "5 5.0\n",
            "4 5.0\n",
            "5 5.0\n",
            "4 5.0\n",
            "5 5.0\n",
            "4 5.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(y_pred)):\n",
        "  print(y_pred[i], labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC-zZ90E-q_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f051df3e-3c2e-4b60-d85e-74e10e16ebed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.922920892494929\n"
          ]
        }
      ],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.data import loadlocal_mnist"
      ],
      "metadata": {
        "id": "3Ojd7AHrsJRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimizer.learning_rate"
      ],
      "metadata": {
        "id": "erUOBlW66zd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebeed67-06ee-4dba-8a0c-b0a0697b73bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'current_learning_rate:0' shape=() dtype=float32, numpy=1e-04>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7_p3KfP7PXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}